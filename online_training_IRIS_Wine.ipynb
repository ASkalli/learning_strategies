{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from CMA_obj import CMA_opt\n",
    "from PEPG_obj import PEPG_opt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from SPSA_obj import SPSA_opt\n",
    "from ADAM_opt import AdamOptimizer\n",
    "from PSO_obj import PSO_opt\n",
    "from scipy.interpolate import interp1d\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from NN_utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Online Training of Neural Networks IRIS, Wine\n",
    "\n",
    "- The NN class helper functions and training loop functions are defined in NN_utils, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets\n",
    "X is the input, Y the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iris dataset\n",
    "iris_df = pd.read_csv(\"data\\\\IRIS\\\\iris.csv\")\n",
    "# convert the last column \n",
    "\n",
    "\n",
    "iris_raw = iris_df.values\n",
    "\n",
    "for i in range(len(iris_raw)):\n",
    "    if iris_raw[i,-1] == 'Iris-setosa':\n",
    "        iris_raw[i,-1] = 0\n",
    "    elif iris_raw[i,-1] == 'Iris-versicolor':\n",
    "        iris_raw[i,-1] = 1\n",
    "    else:\n",
    "        iris_raw[i,-1] = 2\n",
    "        \n",
    "iris_raw = iris_raw.astype(np.float32)\n",
    "#iris raw needs to be shuffled randomly because the data is ordered by class\n",
    "np.random.shuffle(iris_raw)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.from_numpy(iris_raw[:, :-1])\n",
    "Y = torch.from_numpy(iris_raw[:, -1]).unsqueeze(1)\n",
    "\n",
    "# Create a single dataset\n",
    "full_dataset = Custom_dataset(X, Y)\n",
    "\n",
    "# Split into train and test sets, first set the size of the split\n",
    "train_size = int(0.75 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "# split into train and test sets using pytorch randomsplit\n",
    "Iris_train, Iris_test = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "Iris_train_loader = torch.utils.data.DataLoader(dataset=Iris_train, batch_size=train_size, shuffle=True)\n",
    "Iris_test_loader = torch.utils.data.DataLoader(dataset=Iris_test, batch_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine dataset\n",
    "wine_df = pd.read_csv(\"data\\\\WINE\\\\winequality-red.csv\")\n",
    "\n",
    "wine_raw = wine_df.values.astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.from_numpy(wine_raw[:, :-1])\n",
    "Y = torch.from_numpy(wine_raw[:, -1]).unsqueeze(1)\n",
    "\n",
    "# Create a single dataset\n",
    "full_dataset = Custom_dataset(X, Y)\n",
    "\n",
    "# Split into train and test sets, first set the size of the split\n",
    "train_size = int(0.75 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "# split into train and test sets using pytorch randomsplit\n",
    "\n",
    "Wine_train, Wine_test = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "Wine_train_loader = torch.utils.data.DataLoader(dataset=Wine_train, batch_size=100, shuffle=True)\n",
    "Wine_test_loader = torch.utils.data.DataLoader(dataset=Wine_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Neural_Net(\n",
      "  (NN_stack): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=11, out_features=10, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=10, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#We Now create an instance of the NN class and move if to the GPU if available\n",
    "n_neurons = 10\n",
    "NN_Wine = Neural_Net(input_size=11, hidden_size=n_neurons, n_classes=1)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(NN_Wine.parameters(), lr=0.001)\n",
    "\n",
    "#training the full NN\n",
    "n_epochs = 1000\n",
    "test_acc = train_pytorch_NN(NN_Wine, n_epochs, Wine_train_loader, Wine_test_loader, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(test_acc)), y=test_acc, mode='lines', name='Full NN'))\n",
    "fig.update_layout(template='plotly_white', width=400, height=400,margin=dict(l=20, r=20, t=20, b=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop PEPG for MNIST: \n",
    "\n",
    "\n",
    "#NN_MNIST.reset_weights()\n",
    "#NN_MNIST.NN_stack[0].requires_grad = True\n",
    "n_epochs =10\n",
    "NN_MNIST = Tiny_convnet()\n",
    "N_dim = NN_MNIST.count_parameters()\n",
    "pop_size = 200\n",
    "\n",
    "#specify we don't need the computation graph to keep track of the gradients, we will use pepg to update the weights\n",
    "with torch.no_grad():\n",
    "    for param in NN_MNIST.parameters():\n",
    "        param.requires_grad = False\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# learning parameters\n",
    "\n",
    "\n",
    "init_pos = NN_MNIST.get_params()\n",
    "\n",
    "if init_pos.requires_grad:\n",
    "    # Detach the tensor from the computation graph\n",
    "    init_pos = init_pos.detach()\n",
    "if init_pos.is_cuda:\n",
    "    # Move the tensor to the CPU\n",
    "    init_pos = init_pos.cpu()\n",
    "init_pos = init_pos.numpy()\n",
    "\n",
    "PEPG_optimizer = PEPG_opt(N_dim, pop_size, learning_rate=0.01, starting_mu=init_pos ,starting_sigma=0.1)\n",
    "\n",
    "PEPG_optimizer.sigma_decay = 0.9999\n",
    "PEPG_optimizer.sigma_alpha=0.2\n",
    "PEPG_optimizer.sigma_limit=0.02\n",
    "PEPG_optimizer.elite_ratio=0.1\n",
    "PEPG_optimizer.weight_decay=0.005\n",
    "\n",
    "test_acc_PEPG,best_reward_PEPG = train_online_pop_NN(NN_MNIST, n_epochs, train_loader_MNIST, test_loader_MNIST, loss, PEPG_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(test_acc_PEPG)), y=test_acc_PEPG, mode='lines', name='PEPG'))\n",
    "#change theme to white and set the sizer of the plot\n",
    "fig.update_layout(template='plotly_white', width=400, height=300,margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig.update_xaxes(title_text=\"Epochs\",type = 'log')\n",
    "fig.update_yaxes(title_text=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pepg data\n",
    "savetxt('data\\\\Results\\\\NN_training\\\\online_training\\\\PEPG_test_acc.csv', test_acc_PEPG, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use CMA to train the FFNN\n",
    "- This doesn't work at all this simple architecture has too many parameters ... so CMA is painfully slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using CMA-ES for training the NN\n",
    "n_epochs =10\n",
    "NN_MNIST = Tiny_convnet()\n",
    "N_dim = NN_MNIST.count_parameters()\n",
    "pop_size = 200\n",
    "#specify we don't need the computation graph to keep track of the gradients, we will use CMAES to update the weights\n",
    "with torch.no_grad():\n",
    "    for param in NN_MNIST.parameters():\n",
    "        param.requires_grad = False\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# learning parameters\n",
    "\n",
    "init_pos = NN_MNIST.get_params()\n",
    "\n",
    "if init_pos.requires_grad:\n",
    "    # Detach the tensor from the computation graph\n",
    "    init_pos = init_pos.detach()\n",
    "if init_pos.is_cuda:\n",
    "    # Move the tensor to the CPU\n",
    "    init_pos = init_pos.cpu()\n",
    "init_pos = init_pos.numpy()\n",
    "\n",
    "CMA_optimizer = CMA_opt(N_dim, pop_size, select_pop=int(pop_size/2), sigma_init=0.1, mean_init=init_pos)\n",
    "CMA_optimizer.eigen_update_frequency = 10\n",
    "\n",
    "test_acc_CMA,best_reward_CMA = train_online_pop_NN(NN_MNIST, n_epochs, train_loader_MNIST, test_loader_MNIST, loss, CMA_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(test_acc_CMA)), y=test_acc_CMA, mode='lines', name='PEPG'))\n",
    "#change theme to white and set the sizer of the plot\n",
    "fig.update_layout(template='plotly_white', width=400, height=300,margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig.update_xaxes(title_text=\"Epochs\",type = 'log')\n",
    "fig.update_yaxes(title_text=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use SPSA to optimize The Neural network\n",
    "n_epochs =10\n",
    "NN_MNIST = Tiny_convnet()\n",
    "N_dim = NN_MNIST.count_parameters()\n",
    "#specify we don't need the computation graph to keep track of the gradients, we will use SPSA to update the weights\n",
    "with torch.no_grad():\n",
    "    for param in NN_MNIST.parameters():\n",
    "        param.requires_grad = False\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# learning parameters\n",
    "\n",
    "init_pos = NN_MNIST.get_params()\n",
    "\n",
    "if init_pos.requires_grad:\n",
    "    # Detach the tensor from the computation graph\n",
    "    init_pos = init_pos.detach()\n",
    "if init_pos.is_cuda:\n",
    "    # Move the tensor to the CPU\n",
    "    init_pos = init_pos.cpu()\n",
    "init_pos = init_pos.numpy()\n",
    "\n",
    "SPSA_optimizer = SPSA_opt(init_pos,alpha=1e-3,epsilon=1e-7)\n",
    "Adam = AdamOptimizer(init_pos, lr=1e-3, beta1=0.9, beta2=0.99, epsilon=1e-8)\n",
    "\n",
    "test_acc_SPSA, best_reward_SPSA = train_online_SPSA_NN(NN_MNIST, n_epochs, train_loader_MNIST, test_loader_MNIST, loss, SPSA_optimizer,Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(test_acc_SPSA)), y=test_acc_SPSA, mode='lines', name='PEPG'))\n",
    "#change theme to white and set the sizer of the plot\n",
    "fig.update_layout(template='plotly_white', width=400, height=300,margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig.update_xaxes(title_text=\"Epochs\",type = 'log')\n",
    "fig.update_yaxes(title_text=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using PSO for training the NN\n",
    "n_epochs =1\n",
    "NN_MNIST = Tiny_convnet()\n",
    "N_dim = NN_MNIST.count_parameters()\n",
    "pop_size = 100\n",
    "#specify we don't need the computation graph to keep track of the gradients, we will use CMAES to update the weights\n",
    "with torch.no_grad():\n",
    "    for param in NN_MNIST.parameters():\n",
    "        param.requires_grad = False\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# learning parameters\n",
    "\n",
    "init_pos = NN_MNIST.get_params()\n",
    "\n",
    "if init_pos.requires_grad:\n",
    "    # Detach the tensor from the computation graph\n",
    "    init_pos = init_pos.detach()\n",
    "if init_pos.is_cuda:\n",
    "    # Move the tensor to the CPU\n",
    "    init_pos = init_pos.cpu()\n",
    "init_pos = init_pos.numpy()\n",
    "\n",
    "#params dictionary\n",
    "upper_bound = 0.5\n",
    "lower_bound = -0.5\n",
    "\n",
    "params = {'c_1': 2.5, \n",
    "          'c_2': 0.85,\n",
    "          'w': 0.7,\n",
    "          'Vmax': 0.15*(upper_bound-lower_bound),\n",
    "          'upper_bound': upper_bound,\n",
    "          'lower_bound': lower_bound,\n",
    "          'pop_size' :pop_size,\n",
    "          }\n",
    "\n",
    "init_pos = (upper_bound - lower_bound) * np.random.rand(N_dim, pop_size) + lower_bound\n",
    "V_init = 0.1 * np.random.rand(N_dim, pop_size)\n",
    "PSO_optimizer = PSO_opt(X_init = init_pos,V_init = V_init,params=params)\n",
    "\n",
    "test_acc_PSO,best_reward_PSO = train_online_pop_NN(NN_MNIST, n_epochs, train_loader_MNIST, test_loader_MNIST, loss, PSO_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(test_acc_PSO)), y=test_acc_PSO, mode='lines', name='PEPG'))\n",
    "#change theme to white and set the sizer of the plot\n",
    "fig.update_layout(template='plotly_white', width=400, height=300,margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig.update_xaxes(title_text=\"Epochs\",type = 'log')\n",
    "fig.update_yaxes(title_text=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_MNIST\n",
    "\n",
    "# Initialize a list to store the figures\n",
    "figs = []\n",
    "\n",
    "# Iterate through each model parameter\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:  # Filter out only weight parameters\n",
    "        # Flatten the weights\n",
    "        weights = param.detach().cpu().numpy().flatten()\n",
    "        \n",
    "        # Create a histogram for the weights\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Histogram(x=weights, name=name))\n",
    "        \n",
    "        # Update layout to add titles and improve readability\n",
    "        fig.update_layout(\n",
    "            title=f'Histogram of Weights for Layer: {name}',\n",
    "            xaxis_title='Weight values',\n",
    "            yaxis_title='Frequency',\n",
    "            bargap=0.2\n",
    "        )\n",
    "        \n",
    "        # Append the figure to the list\n",
    "        figs.append(fig)\n",
    "\n",
    "# Show all histograms\n",
    "for fig in figs:\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
