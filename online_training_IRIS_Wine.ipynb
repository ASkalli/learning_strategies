{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from CMA_obj import CMA_opt\n",
    "from PEPG_obj import PEPG_opt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from SPSA_obj import SPSA_opt\n",
    "from Finite_diff_grad import FD_opt\n",
    "from ADAM_opt import AdamOptimizer\n",
    "from PSO_obj import PSO_opt\n",
    "from scipy.interpolate import interp1d\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from NN_utils_IRIS import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Online Training of Neural Networks IRIS, Wine\n",
    "\n",
    "- The NN class helper functions and training loop functions are defined in NN_utils, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets\n",
    "X is the input, Y the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\PhD\\simulation\\simulation_python\\learning_strategies\\NN_utils_IRIS.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.features = torch.tensor(features, dtype=torch.float)\n",
      "c:\\Users\\Admin\\Desktop\\PhD\\simulation\\simulation_python\\learning_strategies\\NN_utils_IRIS.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long).squeeze()  # Convert and squeeze labels\n"
     ]
    }
   ],
   "source": [
    "#Iris dataset\n",
    "iris_df = pd.read_csv(\"data\\\\IRIS\\\\iris.csv\")\n",
    "# convert the last column \n",
    "\n",
    "\n",
    "iris_raw = iris_df.values\n",
    "\n",
    "for i in range(len(iris_raw)):\n",
    "    if iris_raw[i,-1] == 'Iris-setosa':\n",
    "        iris_raw[i,-1] = 0\n",
    "    elif iris_raw[i,-1] == 'Iris-versicolor':\n",
    "        iris_raw[i,-1] = 1\n",
    "    else:\n",
    "        iris_raw[i,-1] = 2\n",
    "        \n",
    "iris_raw = iris_raw.astype(np.float32)\n",
    "#remove the first column because it is just an index\n",
    "iris_raw = iris_raw[:,1:]\n",
    "#iris raw needs to be shuffled randomly because the data is ordered by class\n",
    "np.random.shuffle(iris_raw)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.from_numpy(iris_raw[:, :-1])\n",
    "Y = torch.from_numpy(iris_raw[:, -1]).unsqueeze(1)\n",
    "\n",
    "# Create a single dataset\n",
    "full_dataset = Custom_dataset(X, Y)\n",
    "\n",
    "# Split into train and test sets, first set the size of the split\n",
    "train_size = int(0.75 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "# split into train and test sets using pytorch randomsplit\n",
    "Iris_train, Iris_test = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "Iris_train_loader = torch.utils.data.DataLoader(dataset=Iris_train, batch_size=train_size, shuffle=True)\n",
    "Iris_test_loader = torch.utils.data.DataLoader(dataset=Iris_test, batch_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Iris_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Neural_Net(\n",
      "  (NN_stack): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=4, out_features=10, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/200], Step [1/1], Loss: 1.068480372428894, Test Accuracy: 21.05263157894737%\n",
      "Epoch [2/200], Step [1/1], Loss: 1.0603891611099243, Test Accuracy: 21.05263157894737%\n",
      "Epoch [3/200], Step [1/1], Loss: 1.0526516437530518, Test Accuracy: 21.05263157894737%\n",
      "Epoch [4/200], Step [1/1], Loss: 1.0452842712402344, Test Accuracy: 21.05263157894737%\n",
      "Epoch [5/200], Step [1/1], Loss: 1.0383018255233765, Test Accuracy: 21.05263157894737%\n",
      "Epoch [6/200], Step [1/1], Loss: 1.0317190885543823, Test Accuracy: 21.05263157894737%\n",
      "Epoch [7/200], Step [1/1], Loss: 1.0255411863327026, Test Accuracy: 21.05263157894737%\n",
      "Epoch [8/200], Step [1/1], Loss: 1.0197765827178955, Test Accuracy: 21.05263157894737%\n",
      "Epoch [9/200], Step [1/1], Loss: 1.0144295692443848, Test Accuracy: 21.05263157894737%\n",
      "Epoch [10/200], Step [1/1], Loss: 1.0095030069351196, Test Accuracy: 23.68421052631579%\n",
      "Epoch [11/200], Step [1/1], Loss: 1.0049898624420166, Test Accuracy: 28.94736842105263%\n",
      "Epoch [12/200], Step [1/1], Loss: 1.0008851289749146, Test Accuracy: 31.57894736842105%\n",
      "Epoch [13/200], Step [1/1], Loss: 0.9971705675125122, Test Accuracy: 36.8421052631579%\n",
      "Epoch [14/200], Step [1/1], Loss: 0.993825376033783, Test Accuracy: 50.0%\n",
      "Epoch [15/200], Step [1/1], Loss: 0.990822970867157, Test Accuracy: 52.63157894736842%\n",
      "Epoch [16/200], Step [1/1], Loss: 0.9881299734115601, Test Accuracy: 52.63157894736842%\n",
      "Epoch [17/200], Step [1/1], Loss: 0.9857138991355896, Test Accuracy: 55.26315789473684%\n",
      "Epoch [18/200], Step [1/1], Loss: 0.983534038066864, Test Accuracy: 55.26315789473684%\n",
      "Epoch [19/200], Step [1/1], Loss: 0.9815543293952942, Test Accuracy: 50.0%\n",
      "Epoch [20/200], Step [1/1], Loss: 0.9797356724739075, Test Accuracy: 50.0%\n",
      "Epoch [21/200], Step [1/1], Loss: 0.9780439138412476, Test Accuracy: 50.0%\n",
      "Epoch [22/200], Step [1/1], Loss: 0.9764440655708313, Test Accuracy: 47.36842105263158%\n",
      "Epoch [23/200], Step [1/1], Loss: 0.9749071002006531, Test Accuracy: 47.36842105263158%\n",
      "Epoch [24/200], Step [1/1], Loss: 0.9734066724777222, Test Accuracy: 47.36842105263158%\n",
      "Epoch [25/200], Step [1/1], Loss: 0.9719208478927612, Test Accuracy: 47.36842105263158%\n",
      "Epoch [26/200], Step [1/1], Loss: 0.9704331159591675, Test Accuracy: 47.36842105263158%\n",
      "Epoch [27/200], Step [1/1], Loss: 0.9689310193061829, Test Accuracy: 42.10526315789474%\n",
      "Epoch [28/200], Step [1/1], Loss: 0.9674058556556702, Test Accuracy: 39.473684210526315%\n",
      "Epoch [29/200], Step [1/1], Loss: 0.965851902961731, Test Accuracy: 39.473684210526315%\n",
      "Epoch [30/200], Step [1/1], Loss: 0.9642669558525085, Test Accuracy: 39.473684210526315%\n",
      "Epoch [31/200], Step [1/1], Loss: 0.9626504778862, Test Accuracy: 39.473684210526315%\n",
      "Epoch [32/200], Step [1/1], Loss: 0.9610041379928589, Test Accuracy: 42.10526315789474%\n",
      "Epoch [33/200], Step [1/1], Loss: 0.9593304395675659, Test Accuracy: 44.73684210526316%\n",
      "Epoch [34/200], Step [1/1], Loss: 0.9576324820518494, Test Accuracy: 47.36842105263158%\n",
      "Epoch [35/200], Step [1/1], Loss: 0.9559149742126465, Test Accuracy: 47.36842105263158%\n",
      "Epoch [36/200], Step [1/1], Loss: 0.9541807174682617, Test Accuracy: 47.36842105263158%\n",
      "Epoch [37/200], Step [1/1], Loss: 0.952433168888092, Test Accuracy: 47.36842105263158%\n",
      "Epoch [38/200], Step [1/1], Loss: 0.9506754279136658, Test Accuracy: 47.36842105263158%\n",
      "Epoch [39/200], Step [1/1], Loss: 0.9489091038703918, Test Accuracy: 47.36842105263158%\n",
      "Epoch [40/200], Step [1/1], Loss: 0.9471355080604553, Test Accuracy: 47.36842105263158%\n",
      "Epoch [41/200], Step [1/1], Loss: 0.9453552961349487, Test Accuracy: 47.36842105263158%\n",
      "Epoch [42/200], Step [1/1], Loss: 0.9435680508613586, Test Accuracy: 47.36842105263158%\n",
      "Epoch [43/200], Step [1/1], Loss: 0.9417729377746582, Test Accuracy: 50.0%\n",
      "Epoch [44/200], Step [1/1], Loss: 0.9399682879447937, Test Accuracy: 50.0%\n",
      "Epoch [45/200], Step [1/1], Loss: 0.9381532073020935, Test Accuracy: 50.0%\n",
      "Epoch [46/200], Step [1/1], Loss: 0.9363251328468323, Test Accuracy: 50.0%\n",
      "Epoch [47/200], Step [1/1], Loss: 0.9344824552536011, Test Accuracy: 50.0%\n",
      "Epoch [48/200], Step [1/1], Loss: 0.9326236844062805, Test Accuracy: 50.0%\n",
      "Epoch [49/200], Step [1/1], Loss: 0.9307488799095154, Test Accuracy: 50.0%\n",
      "Epoch [50/200], Step [1/1], Loss: 0.9288564920425415, Test Accuracy: 50.0%\n",
      "Epoch [51/200], Step [1/1], Loss: 0.9269464612007141, Test Accuracy: 50.0%\n",
      "Epoch [52/200], Step [1/1], Loss: 0.9250170588493347, Test Accuracy: 50.0%\n",
      "Epoch [53/200], Step [1/1], Loss: 0.9230672717094421, Test Accuracy: 50.0%\n",
      "Epoch [54/200], Step [1/1], Loss: 0.9210972785949707, Test Accuracy: 50.0%\n",
      "Epoch [55/200], Step [1/1], Loss: 0.9191073775291443, Test Accuracy: 50.0%\n",
      "Epoch [56/200], Step [1/1], Loss: 0.9170985221862793, Test Accuracy: 50.0%\n",
      "Epoch [57/200], Step [1/1], Loss: 0.9150719046592712, Test Accuracy: 50.0%\n",
      "Epoch [58/200], Step [1/1], Loss: 0.913026750087738, Test Accuracy: 50.0%\n",
      "Epoch [59/200], Step [1/1], Loss: 0.9109637141227722, Test Accuracy: 50.0%\n",
      "Epoch [60/200], Step [1/1], Loss: 0.9088834524154663, Test Accuracy: 50.0%\n",
      "Epoch [61/200], Step [1/1], Loss: 0.9067863821983337, Test Accuracy: 50.0%\n",
      "Epoch [62/200], Step [1/1], Loss: 0.9046725630760193, Test Accuracy: 50.0%\n",
      "Epoch [63/200], Step [1/1], Loss: 0.9025424718856812, Test Accuracy: 50.0%\n",
      "Epoch [64/200], Step [1/1], Loss: 0.9003960490226746, Test Accuracy: 50.0%\n",
      "Epoch [65/200], Step [1/1], Loss: 0.8982332944869995, Test Accuracy: 50.0%\n",
      "Epoch [66/200], Step [1/1], Loss: 0.8960541486740112, Test Accuracy: 50.0%\n",
      "Epoch [67/200], Step [1/1], Loss: 0.8938583731651306, Test Accuracy: 50.0%\n",
      "Epoch [68/200], Step [1/1], Loss: 0.8916458487510681, Test Accuracy: 50.0%\n",
      "Epoch [69/200], Step [1/1], Loss: 0.8894163966178894, Test Accuracy: 50.0%\n",
      "Epoch [70/200], Step [1/1], Loss: 0.8871698379516602, Test Accuracy: 50.0%\n",
      "Epoch [71/200], Step [1/1], Loss: 0.8849064707756042, Test Accuracy: 50.0%\n",
      "Epoch [72/200], Step [1/1], Loss: 0.8826258778572083, Test Accuracy: 50.0%\n",
      "Epoch [73/200], Step [1/1], Loss: 0.8803282976150513, Test Accuracy: 50.0%\n",
      "Epoch [74/200], Step [1/1], Loss: 0.8780292868614197, Test Accuracy: 50.0%\n",
      "Epoch [75/200], Step [1/1], Loss: 0.8757190704345703, Test Accuracy: 50.0%\n",
      "Epoch [76/200], Step [1/1], Loss: 0.8733944892883301, Test Accuracy: 50.0%\n",
      "Epoch [77/200], Step [1/1], Loss: 0.8710557222366333, Test Accuracy: 52.63157894736842%\n",
      "Epoch [78/200], Step [1/1], Loss: 0.8687030076980591, Test Accuracy: 52.63157894736842%\n",
      "Epoch [79/200], Step [1/1], Loss: 0.8663367033004761, Test Accuracy: 57.89473684210526%\n",
      "Epoch [80/200], Step [1/1], Loss: 0.8639569878578186, Test Accuracy: 63.1578947368421%\n",
      "Epoch [81/200], Step [1/1], Loss: 0.8615638017654419, Test Accuracy: 68.42105263157895%\n",
      "Epoch [82/200], Step [1/1], Loss: 0.8591646552085876, Test Accuracy: 73.6842105263158%\n",
      "Epoch [83/200], Step [1/1], Loss: 0.8567571640014648, Test Accuracy: 76.3157894736842%\n",
      "Epoch [84/200], Step [1/1], Loss: 0.8543383479118347, Test Accuracy: 81.57894736842105%\n",
      "Epoch [85/200], Step [1/1], Loss: 0.8519085645675659, Test Accuracy: 81.57894736842105%\n",
      "Epoch [86/200], Step [1/1], Loss: 0.8494687080383301, Test Accuracy: 81.57894736842105%\n",
      "Epoch [87/200], Step [1/1], Loss: 0.8470181226730347, Test Accuracy: 86.84210526315789%\n",
      "Epoch [88/200], Step [1/1], Loss: 0.8445571660995483, Test Accuracy: 86.84210526315789%\n",
      "Epoch [89/200], Step [1/1], Loss: 0.84210205078125, Test Accuracy: 86.84210526315789%\n",
      "Epoch [90/200], Step [1/1], Loss: 0.8396704792976379, Test Accuracy: 86.84210526315789%\n",
      "Epoch [91/200], Step [1/1], Loss: 0.8372505307197571, Test Accuracy: 89.47368421052632%\n",
      "Epoch [92/200], Step [1/1], Loss: 0.8348270654678345, Test Accuracy: 92.10526315789474%\n",
      "Epoch [93/200], Step [1/1], Loss: 0.8324011564254761, Test Accuracy: 92.10526315789474%\n",
      "Epoch [94/200], Step [1/1], Loss: 0.8299989700317383, Test Accuracy: 92.10526315789474%\n",
      "Epoch [95/200], Step [1/1], Loss: 0.8275954127311707, Test Accuracy: 92.10526315789474%\n",
      "Epoch [96/200], Step [1/1], Loss: 0.8251875638961792, Test Accuracy: 92.10526315789474%\n",
      "Epoch [97/200], Step [1/1], Loss: 0.8227758407592773, Test Accuracy: 92.10526315789474%\n",
      "Epoch [98/200], Step [1/1], Loss: 0.8203603029251099, Test Accuracy: 92.10526315789474%\n",
      "Epoch [99/200], Step [1/1], Loss: 0.8179556131362915, Test Accuracy: 94.73684210526316%\n",
      "Epoch [100/200], Step [1/1], Loss: 0.8155578374862671, Test Accuracy: 94.73684210526316%\n",
      "Epoch [101/200], Step [1/1], Loss: 0.8131569623947144, Test Accuracy: 94.73684210526316%\n",
      "Epoch [102/200], Step [1/1], Loss: 0.8107605576515198, Test Accuracy: 94.73684210526316%\n",
      "Epoch [103/200], Step [1/1], Loss: 0.80836421251297, Test Accuracy: 94.73684210526316%\n",
      "Epoch [104/200], Step [1/1], Loss: 0.8059702515602112, Test Accuracy: 94.73684210526316%\n",
      "Epoch [105/200], Step [1/1], Loss: 0.8035786747932434, Test Accuracy: 94.73684210526316%\n",
      "Epoch [106/200], Step [1/1], Loss: 0.8011834025382996, Test Accuracy: 94.73684210526316%\n",
      "Epoch [107/200], Step [1/1], Loss: 0.7987851500511169, Test Accuracy: 94.73684210526316%\n",
      "Epoch [108/200], Step [1/1], Loss: 0.7963842153549194, Test Accuracy: 94.73684210526316%\n",
      "Epoch [109/200], Step [1/1], Loss: 0.7939806580543518, Test Accuracy: 94.73684210526316%\n",
      "Epoch [110/200], Step [1/1], Loss: 0.791577160358429, Test Accuracy: 94.73684210526316%\n",
      "Epoch [111/200], Step [1/1], Loss: 0.7891724705696106, Test Accuracy: 94.73684210526316%\n",
      "Epoch [112/200], Step [1/1], Loss: 0.7867655158042908, Test Accuracy: 94.73684210526316%\n",
      "Epoch [113/200], Step [1/1], Loss: 0.7843570709228516, Test Accuracy: 97.36842105263158%\n",
      "Epoch [114/200], Step [1/1], Loss: 0.7819466590881348, Test Accuracy: 97.36842105263158%\n",
      "Epoch [115/200], Step [1/1], Loss: 0.7795344591140747, Test Accuracy: 97.36842105263158%\n",
      "Epoch [116/200], Step [1/1], Loss: 0.7771210074424744, Test Accuracy: 97.36842105263158%\n",
      "Epoch [117/200], Step [1/1], Loss: 0.7747064828872681, Test Accuracy: 97.36842105263158%\n",
      "Epoch [118/200], Step [1/1], Loss: 0.7722919583320618, Test Accuracy: 97.36842105263158%\n",
      "Epoch [119/200], Step [1/1], Loss: 0.769877552986145, Test Accuracy: 97.36842105263158%\n",
      "Epoch [120/200], Step [1/1], Loss: 0.767463207244873, Test Accuracy: 97.36842105263158%\n",
      "Epoch [121/200], Step [1/1], Loss: 0.7650507688522339, Test Accuracy: 97.36842105263158%\n",
      "Epoch [122/200], Step [1/1], Loss: 0.7626396417617798, Test Accuracy: 97.36842105263158%\n",
      "Epoch [123/200], Step [1/1], Loss: 0.7602297067642212, Test Accuracy: 97.36842105263158%\n",
      "Epoch [124/200], Step [1/1], Loss: 0.7578209638595581, Test Accuracy: 97.36842105263158%\n",
      "Epoch [125/200], Step [1/1], Loss: 0.7554135322570801, Test Accuracy: 97.36842105263158%\n",
      "Epoch [126/200], Step [1/1], Loss: 0.7530081868171692, Test Accuracy: 97.36842105263158%\n",
      "Epoch [127/200], Step [1/1], Loss: 0.7506046891212463, Test Accuracy: 97.36842105263158%\n",
      "Epoch [128/200], Step [1/1], Loss: 0.7482035756111145, Test Accuracy: 97.36842105263158%\n",
      "Epoch [129/200], Step [1/1], Loss: 0.7458047866821289, Test Accuracy: 97.36842105263158%\n",
      "Epoch [130/200], Step [1/1], Loss: 0.7434102892875671, Test Accuracy: 97.36842105263158%\n",
      "Epoch [131/200], Step [1/1], Loss: 0.7410200238227844, Test Accuracy: 97.36842105263158%\n",
      "Epoch [132/200], Step [1/1], Loss: 0.7386338114738464, Test Accuracy: 97.36842105263158%\n",
      "Epoch [133/200], Step [1/1], Loss: 0.7362512946128845, Test Accuracy: 97.36842105263158%\n",
      "Epoch [134/200], Step [1/1], Loss: 0.7338722944259644, Test Accuracy: 97.36842105263158%\n",
      "Epoch [135/200], Step [1/1], Loss: 0.7314969897270203, Test Accuracy: 97.36842105263158%\n",
      "Epoch [136/200], Step [1/1], Loss: 0.7291263341903687, Test Accuracy: 97.36842105263158%\n",
      "Epoch [137/200], Step [1/1], Loss: 0.7267599701881409, Test Accuracy: 97.36842105263158%\n",
      "Epoch [138/200], Step [1/1], Loss: 0.7243981957435608, Test Accuracy: 97.36842105263158%\n",
      "Epoch [139/200], Step [1/1], Loss: 0.7220415472984314, Test Accuracy: 97.36842105263158%\n",
      "Epoch [140/200], Step [1/1], Loss: 0.7196906208992004, Test Accuracy: 97.36842105263158%\n",
      "Epoch [141/200], Step [1/1], Loss: 0.7173457741737366, Test Accuracy: 97.36842105263158%\n",
      "Epoch [142/200], Step [1/1], Loss: 0.7150065302848816, Test Accuracy: 97.36842105263158%\n",
      "Epoch [143/200], Step [1/1], Loss: 0.7126733064651489, Test Accuracy: 97.36842105263158%\n",
      "Epoch [144/200], Step [1/1], Loss: 0.7103464007377625, Test Accuracy: 97.36842105263158%\n",
      "Epoch [145/200], Step [1/1], Loss: 0.7080258131027222, Test Accuracy: 100.0%\n",
      "Epoch [146/200], Step [1/1], Loss: 0.7057127952575684, Test Accuracy: 100.0%\n",
      "Epoch [147/200], Step [1/1], Loss: 0.7034071087837219, Test Accuracy: 100.0%\n",
      "Epoch [148/200], Step [1/1], Loss: 0.7011080384254456, Test Accuracy: 100.0%\n",
      "Epoch [149/200], Step [1/1], Loss: 0.6988160014152527, Test Accuracy: 100.0%\n",
      "Epoch [150/200], Step [1/1], Loss: 0.6965314745903015, Test Accuracy: 100.0%\n",
      "Epoch [151/200], Step [1/1], Loss: 0.6942542195320129, Test Accuracy: 100.0%\n",
      "Epoch [152/200], Step [1/1], Loss: 0.6919846534729004, Test Accuracy: 100.0%\n",
      "Epoch [153/200], Step [1/1], Loss: 0.6897233128547668, Test Accuracy: 100.0%\n",
      "Epoch [154/200], Step [1/1], Loss: 0.6874698400497437, Test Accuracy: 100.0%\n",
      "Epoch [155/200], Step [1/1], Loss: 0.6852250099182129, Test Accuracy: 100.0%\n",
      "Epoch [156/200], Step [1/1], Loss: 0.6829890608787537, Test Accuracy: 100.0%\n",
      "Epoch [157/200], Step [1/1], Loss: 0.6807616949081421, Test Accuracy: 100.0%\n",
      "Epoch [158/200], Step [1/1], Loss: 0.6785427331924438, Test Accuracy: 100.0%\n",
      "Epoch [159/200], Step [1/1], Loss: 0.6763325333595276, Test Accuracy: 100.0%\n",
      "Epoch [160/200], Step [1/1], Loss: 0.6741312742233276, Test Accuracy: 100.0%\n",
      "Epoch [161/200], Step [1/1], Loss: 0.6719388961791992, Test Accuracy: 100.0%\n",
      "Epoch [162/200], Step [1/1], Loss: 0.6697555780410767, Test Accuracy: 100.0%\n",
      "Epoch [163/200], Step [1/1], Loss: 0.6675814986228943, Test Accuracy: 100.0%\n",
      "Epoch [164/200], Step [1/1], Loss: 0.6654167771339417, Test Accuracy: 100.0%\n",
      "Epoch [165/200], Step [1/1], Loss: 0.6632615923881531, Test Accuracy: 100.0%\n",
      "Epoch [166/200], Step [1/1], Loss: 0.661115825176239, Test Accuracy: 100.0%\n",
      "Epoch [167/200], Step [1/1], Loss: 0.6589798331260681, Test Accuracy: 100.0%\n",
      "Epoch [168/200], Step [1/1], Loss: 0.6568533182144165, Test Accuracy: 100.0%\n",
      "Epoch [169/200], Step [1/1], Loss: 0.6547368764877319, Test Accuracy: 100.0%\n",
      "Epoch [170/200], Step [1/1], Loss: 0.6526302099227905, Test Accuracy: 100.0%\n",
      "Epoch [171/200], Step [1/1], Loss: 0.6505335569381714, Test Accuracy: 100.0%\n",
      "Epoch [172/200], Step [1/1], Loss: 0.6484469175338745, Test Accuracy: 100.0%\n",
      "Epoch [173/200], Step [1/1], Loss: 0.6463702321052551, Test Accuracy: 100.0%\n",
      "Epoch [174/200], Step [1/1], Loss: 0.6443039178848267, Test Accuracy: 100.0%\n",
      "Epoch [175/200], Step [1/1], Loss: 0.6422477960586548, Test Accuracy: 100.0%\n",
      "Epoch [176/200], Step [1/1], Loss: 0.640201985836029, Test Accuracy: 100.0%\n",
      "Epoch [177/200], Step [1/1], Loss: 0.6381664276123047, Test Accuracy: 100.0%\n",
      "Epoch [178/200], Step [1/1], Loss: 0.6361411213874817, Test Accuracy: 100.0%\n",
      "Epoch [179/200], Step [1/1], Loss: 0.6341263055801392, Test Accuracy: 100.0%\n",
      "Epoch [180/200], Step [1/1], Loss: 0.6321219205856323, Test Accuracy: 100.0%\n",
      "Epoch [181/200], Step [1/1], Loss: 0.630128026008606, Test Accuracy: 100.0%\n",
      "Epoch [182/200], Step [1/1], Loss: 0.6281446814537048, Test Accuracy: 100.0%\n",
      "Epoch [183/200], Step [1/1], Loss: 0.6261717081069946, Test Accuracy: 100.0%\n",
      "Epoch [184/200], Step [1/1], Loss: 0.6242090463638306, Test Accuracy: 100.0%\n",
      "Epoch [185/200], Step [1/1], Loss: 0.6222572922706604, Test Accuracy: 100.0%\n",
      "Epoch [186/200], Step [1/1], Loss: 0.6203158497810364, Test Accuracy: 100.0%\n",
      "Epoch [187/200], Step [1/1], Loss: 0.6183850169181824, Test Accuracy: 100.0%\n",
      "Epoch [188/200], Step [1/1], Loss: 0.6164647340774536, Test Accuracy: 100.0%\n",
      "Epoch [189/200], Step [1/1], Loss: 0.6145548820495605, Test Accuracy: 100.0%\n",
      "Epoch [190/200], Step [1/1], Loss: 0.6126556992530823, Test Accuracy: 100.0%\n",
      "Epoch [191/200], Step [1/1], Loss: 0.6107667684555054, Test Accuracy: 100.0%\n",
      "Epoch [192/200], Step [1/1], Loss: 0.608888566493988, Test Accuracy: 100.0%\n",
      "Epoch [193/200], Step [1/1], Loss: 0.6070207357406616, Test Accuracy: 100.0%\n",
      "Epoch [194/200], Step [1/1], Loss: 0.6051633954048157, Test Accuracy: 100.0%\n",
      "Epoch [195/200], Step [1/1], Loss: 0.6033163666725159, Test Accuracy: 100.0%\n",
      "Epoch [196/200], Step [1/1], Loss: 0.6014798283576965, Test Accuracy: 100.0%\n",
      "Epoch [197/200], Step [1/1], Loss: 0.5996536612510681, Test Accuracy: 100.0%\n",
      "Epoch [198/200], Step [1/1], Loss: 0.5978378057479858, Test Accuracy: 100.0%\n",
      "Epoch [199/200], Step [1/1], Loss: 0.5960322618484497, Test Accuracy: 100.0%\n",
      "Epoch [200/200], Step [1/1], Loss: 0.5942367911338806, Test Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "#We Now create an instance of the NN class and move if to the GPU if available\n",
    "n_neurons = 10\n",
    "NN_IRIS = Neural_Net(input_size=4, hidden_size=n_neurons, n_classes=3)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(NN_IRIS.parameters(), lr=0.001)\n",
    "\n",
    "#training the full NN\n",
    "n_epochs = 200\n",
    "test_acc = train_pytorch_NN(NN_IRIS, n_epochs, Iris_train_loader, Iris_test_loader, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "for i, (images, labels) in enumerate(Iris_train_loader):\n",
    "    print(labels.shape)  # Should print something like torch.Size([batch_size])\n",
    "    print(labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Full NN",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          23.68421052631579,
          28.94736842105263,
          31.57894736842105,
          36.8421052631579,
          50,
          52.63157894736842,
          52.63157894736842,
          55.26315789473684,
          55.26315789473684,
          50,
          50,
          50,
          47.36842105263158,
          47.36842105263158,
          47.36842105263158,
          47.36842105263158,
          47.36842105263158,
          42.10526315789474,
          39.473684210526315,
          39.473684210526315,
          39.473684210526315,
          39.473684210526315,
          42.10526315789474,
          44.73684210526316,
          47.36842105263158,
          47.36842105263158,
          47.36842105263158,
          47.36842105263158,
          47.36842105263158,
          47.36842105263158,
          47.36842105263158,
          47.36842105263158,
          47.36842105263158,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          52.63157894736842,
          52.63157894736842,
          57.89473684210526,
          63.1578947368421,
          68.42105263157895,
          73.6842105263158,
          76.3157894736842,
          81.57894736842105,
          81.57894736842105,
          81.57894736842105,
          86.84210526315789,
          86.84210526315789,
          86.84210526315789,
          86.84210526315789,
          89.47368421052632,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100
         ]
        }
       ],
       "layout": {
        "height": 400,
        "margin": {
         "b": 20,
         "l": 20,
         "r": 20,
         "t": 20
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "width": 400
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(test_acc)), y=test_acc, mode='lines', name='Full NN'))\n",
    "fig.update_layout(template='plotly_white', width=400, height=400,margin=dict(l=20, r=20, t=20, b=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Neural_Net(\n",
      "  (NN_stack): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=4, out_features=10, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "{i+1}Epoch [1/100], Step [1/1], Loss: 0.9452974200248718, Test Accuracy: 60.526315789473685%\n",
      "{i+1}Epoch [2/100], Step [1/1], Loss: 0.8118532299995422, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [3/100], Step [1/1], Loss: 0.9389926791191101, Test Accuracy: 63.1578947368421%\n",
      "{i+1}Epoch [4/100], Step [1/1], Loss: 1.0415397882461548, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [5/100], Step [1/1], Loss: 1.02104914188385, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [6/100], Step [1/1], Loss: 1.029732584953308, Test Accuracy: 31.57894736842105%\n",
      "{i+1}Epoch [7/100], Step [1/1], Loss: 0.8549807667732239, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [8/100], Step [1/1], Loss: 0.9713361859321594, Test Accuracy: 60.526315789473685%\n",
      "{i+1}Epoch [9/100], Step [1/1], Loss: 0.8789290189743042, Test Accuracy: 63.1578947368421%\n",
      "{i+1}Epoch [10/100], Step [1/1], Loss: 0.9384482502937317, Test Accuracy: 36.8421052631579%\n",
      "{i+1}Epoch [11/100], Step [1/1], Loss: 0.9116944670677185, Test Accuracy: 65.78947368421052%\n",
      "{i+1}Epoch [12/100], Step [1/1], Loss: 0.8801676034927368, Test Accuracy: 73.6842105263158%\n",
      "{i+1}Epoch [13/100], Step [1/1], Loss: 0.9069814085960388, Test Accuracy: 42.10526315789474%\n",
      "{i+1}Epoch [14/100], Step [1/1], Loss: 0.8490195274353027, Test Accuracy: 65.78947368421052%\n",
      "{i+1}Epoch [15/100], Step [1/1], Loss: 0.808035671710968, Test Accuracy: 65.78947368421052%\n",
      "{i+1}Epoch [16/100], Step [1/1], Loss: 0.8574537038803101, Test Accuracy: 47.36842105263158%\n",
      "{i+1}Epoch [17/100], Step [1/1], Loss: 0.8164932131767273, Test Accuracy: 65.78947368421052%\n",
      "{i+1}Epoch [18/100], Step [1/1], Loss: 0.8955089449882507, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [19/100], Step [1/1], Loss: 0.8064907193183899, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [20/100], Step [1/1], Loss: 0.8727065920829773, Test Accuracy: 26.31578947368421%\n",
      "{i+1}Epoch [21/100], Step [1/1], Loss: 0.7527596354484558, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [22/100], Step [1/1], Loss: 0.765631914138794, Test Accuracy: 68.42105263157895%\n",
      "{i+1}Epoch [23/100], Step [1/1], Loss: 0.8629752993583679, Test Accuracy: 65.78947368421052%\n",
      "{i+1}Epoch [24/100], Step [1/1], Loss: 0.7440223097801208, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [25/100], Step [1/1], Loss: 0.7593188881874084, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [26/100], Step [1/1], Loss: 0.7694185972213745, Test Accuracy: 63.1578947368421%\n",
      "{i+1}Epoch [27/100], Step [1/1], Loss: 0.6917739510536194, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [28/100], Step [1/1], Loss: 0.7919570207595825, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [29/100], Step [1/1], Loss: 0.7974278330802917, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [30/100], Step [1/1], Loss: 0.7756457328796387, Test Accuracy: 71.05263157894737%\n",
      "{i+1}Epoch [31/100], Step [1/1], Loss: 0.8037531971931458, Test Accuracy: 68.42105263157895%\n",
      "{i+1}Epoch [32/100], Step [1/1], Loss: 0.7856547236442566, Test Accuracy: 63.1578947368421%\n",
      "{i+1}Epoch [33/100], Step [1/1], Loss: 0.6867547631263733, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [34/100], Step [1/1], Loss: 0.7307790517807007, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [35/100], Step [1/1], Loss: 0.6999745965003967, Test Accuracy: 71.05263157894737%\n",
      "{i+1}Epoch [36/100], Step [1/1], Loss: 0.642379879951477, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [37/100], Step [1/1], Loss: 0.702703058719635, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [38/100], Step [1/1], Loss: 0.6489347815513611, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [39/100], Step [1/1], Loss: 0.665509819984436, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [40/100], Step [1/1], Loss: 0.5489168763160706, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [41/100], Step [1/1], Loss: 0.6128550171852112, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [42/100], Step [1/1], Loss: 0.7169966697692871, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [43/100], Step [1/1], Loss: 0.6499050855636597, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [44/100], Step [1/1], Loss: 0.5903993248939514, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [45/100], Step [1/1], Loss: 0.566476047039032, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [46/100], Step [1/1], Loss: 0.578274130821228, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [47/100], Step [1/1], Loss: 0.5512627959251404, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [48/100], Step [1/1], Loss: 0.5945071578025818, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [49/100], Step [1/1], Loss: 0.6395754218101501, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [50/100], Step [1/1], Loss: 0.5321754217147827, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [51/100], Step [1/1], Loss: 0.5539892315864563, Test Accuracy: 68.42105263157895%\n",
      "{i+1}Epoch [52/100], Step [1/1], Loss: 0.5751859545707703, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [53/100], Step [1/1], Loss: 0.5726321935653687, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [54/100], Step [1/1], Loss: 0.5374148488044739, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [55/100], Step [1/1], Loss: 0.5247426629066467, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [56/100], Step [1/1], Loss: 0.5194888114929199, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [57/100], Step [1/1], Loss: 0.6019513010978699, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [58/100], Step [1/1], Loss: 0.5260653495788574, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [59/100], Step [1/1], Loss: 0.5072658658027649, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [60/100], Step [1/1], Loss: 0.4794161915779114, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [61/100], Step [1/1], Loss: 0.47967520356178284, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [62/100], Step [1/1], Loss: 0.5610941052436829, Test Accuracy: 68.42105263157895%\n",
      "{i+1}Epoch [63/100], Step [1/1], Loss: 0.4495182931423187, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [64/100], Step [1/1], Loss: 0.4780285954475403, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [65/100], Step [1/1], Loss: 0.501585066318512, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [66/100], Step [1/1], Loss: 0.46803995966911316, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [67/100], Step [1/1], Loss: 0.4382108747959137, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [68/100], Step [1/1], Loss: 0.4816674292087555, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [69/100], Step [1/1], Loss: 0.47729477286338806, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [70/100], Step [1/1], Loss: 0.4470343291759491, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [71/100], Step [1/1], Loss: 0.44342201948165894, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [72/100], Step [1/1], Loss: 0.45649176836013794, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [73/100], Step [1/1], Loss: 0.4374399781227112, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [74/100], Step [1/1], Loss: 0.4516478478908539, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [75/100], Step [1/1], Loss: 0.4881543219089508, Test Accuracy: 65.78947368421052%\n",
      "{i+1}Epoch [76/100], Step [1/1], Loss: 0.4446834325790405, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [77/100], Step [1/1], Loss: 0.45975378155708313, Test Accuracy: 68.42105263157895%\n",
      "{i+1}Epoch [78/100], Step [1/1], Loss: 0.4240349233150482, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [79/100], Step [1/1], Loss: 0.4483003616333008, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [80/100], Step [1/1], Loss: 0.4834461212158203, Test Accuracy: 68.42105263157895%\n",
      "{i+1}Epoch [81/100], Step [1/1], Loss: 0.45987972617149353, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [82/100], Step [1/1], Loss: 0.4253983199596405, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [83/100], Step [1/1], Loss: 0.44412538409233093, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [84/100], Step [1/1], Loss: 0.44183215498924255, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [85/100], Step [1/1], Loss: 0.4889344274997711, Test Accuracy: 76.3157894736842%\n",
      "{i+1}Epoch [86/100], Step [1/1], Loss: 0.4300035536289215, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [87/100], Step [1/1], Loss: 0.3960600197315216, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [88/100], Step [1/1], Loss: 0.3776184022426605, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [89/100], Step [1/1], Loss: 0.38485097885131836, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [90/100], Step [1/1], Loss: 0.43534380197525024, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [91/100], Step [1/1], Loss: 0.4124883711338043, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [92/100], Step [1/1], Loss: 0.42240259051322937, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [93/100], Step [1/1], Loss: 0.3440053462982178, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [94/100], Step [1/1], Loss: 0.38868045806884766, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [95/100], Step [1/1], Loss: 0.36959007382392883, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [96/100], Step [1/1], Loss: 0.4425601363182068, Test Accuracy: 73.6842105263158%\n",
      "{i+1}Epoch [97/100], Step [1/1], Loss: 0.4226320683956146, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [98/100], Step [1/1], Loss: 0.4009188115596771, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [99/100], Step [1/1], Loss: 0.3997112810611725, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [100/100], Step [1/1], Loss: 0.38168883323669434, Test Accuracy: 84.21052631578948%\n"
     ]
    }
   ],
   "source": [
    "# Training loop PEPG for MNIST: \n",
    "\n",
    "\n",
    "#NN_MNIST.reset_weights()\n",
    "#NN_MNIST.NN_stack[0].requires_grad = True\n",
    "n_epochs =100\n",
    "NN_IRIS = Neural_Net(input_size=4, hidden_size=10, n_classes=3)\n",
    "N_dim = NN_IRIS.count_parameters()\n",
    "pop_size = 100\n",
    "\n",
    "#specify we don't need the computation graph to keep track of the gradients, we will use pepg to update the weights\n",
    "with torch.no_grad():\n",
    "    for param in NN_IRIS.parameters():\n",
    "        param.requires_grad = False\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# learning parameters\n",
    "\n",
    "\n",
    "init_pos = NN_IRIS.get_params()\n",
    "\n",
    "if init_pos.requires_grad:\n",
    "    # Detach the tensor from the computation graph\n",
    "    init_pos = init_pos.detach()\n",
    "if init_pos.is_cuda:\n",
    "    # Move the tensor to the CPU\n",
    "    init_pos = init_pos.cpu()\n",
    "init_pos = init_pos.numpy()\n",
    "\n",
    "PEPG_optimizer = PEPG_opt(N_dim, pop_size, learning_rate=0.01, starting_mu=init_pos ,starting_sigma=0.1)\n",
    "\n",
    "PEPG_optimizer.sigma_decay = 0.9999\n",
    "PEPG_optimizer.sigma_alpha=0.2\n",
    "PEPG_optimizer.sigma_limit=0.02\n",
    "PEPG_optimizer.elite_ratio=0.1\n",
    "PEPG_optimizer.weight_decay=0.005\n",
    "\n",
    "test_acc_PEPG,best_reward_PEPG = train_online_pop_NN(NN_IRIS, n_epochs, Iris_train_loader, Iris_test_loader, loss, PEPG_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "PEPG",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          60.526315789473685,
          78.94736842105263,
          63.1578947368421,
          21.05263157894737,
          21.05263157894737,
          31.57894736842105,
          78.94736842105263,
          60.526315789473685,
          63.1578947368421,
          36.8421052631579,
          65.78947368421052,
          73.6842105263158,
          42.10526315789474,
          65.78947368421052,
          65.78947368421052,
          47.36842105263158,
          65.78947368421052,
          34.21052631578947,
          78.94736842105263,
          26.31578947368421,
          78.94736842105263,
          68.42105263157895,
          65.78947368421052,
          78.94736842105263,
          89.47368421052632,
          63.1578947368421,
          78.94736842105263,
          86.84210526315789,
          78.94736842105263,
          71.05263157894737,
          68.42105263157895,
          63.1578947368421,
          81.57894736842105,
          84.21052631578948,
          71.05263157894737,
          78.94736842105263,
          78.94736842105263,
          78.94736842105263,
          84.21052631578948,
          86.84210526315789,
          86.84210526315789,
          81.57894736842105,
          84.21052631578948,
          78.94736842105263,
          78.94736842105263,
          78.94736842105263,
          78.94736842105263,
          78.94736842105263,
          89.47368421052632,
          94.73684210526316,
          68.42105263157895,
          78.94736842105263,
          78.94736842105263,
          86.84210526315789,
          94.73684210526316,
          86.84210526315789,
          89.47368421052632,
          100,
          78.94736842105263,
          81.57894736842105,
          84.21052631578948,
          68.42105263157895,
          78.94736842105263,
          92.10526315789474,
          84.21052631578948,
          92.10526315789474,
          94.73684210526316,
          78.94736842105263,
          86.84210526315789,
          84.21052631578948,
          81.57894736842105,
          92.10526315789474,
          100,
          100,
          65.78947368421052,
          97.36842105263158,
          68.42105263157895,
          100,
          100,
          68.42105263157895,
          89.47368421052632,
          81.57894736842105,
          86.84210526315789,
          78.94736842105263,
          76.3157894736842,
          89.47368421052632,
          94.73684210526316,
          92.10526315789474,
          97.36842105263158,
          94.73684210526316,
          92.10526315789474,
          81.57894736842105,
          84.21052631578948,
          97.36842105263158,
          94.73684210526316,
          73.6842105263158,
          81.57894736842105,
          94.73684210526316,
          92.10526315789474,
          84.21052631578948
         ]
        }
       ],
       "layout": {
        "height": 300,
        "margin": {
         "b": 20,
         "l": 20,
         "r": 20,
         "t": 20
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "width": 400,
        "xaxis": {
         "title": {
          "text": "Epochs"
         },
         "type": "log"
        },
        "yaxis": {
         "title": {
          "text": "Accuracy [%]"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(test_acc_PEPG)), y=test_acc_PEPG, mode='lines', name='PEPG'))\n",
    "#change theme to white and set the sizer of the plot\n",
    "fig.update_layout(template='plotly_white', width=400, height=300,margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig.update_xaxes(title_text=\"Epochs\",type = 'log')\n",
    "fig.update_yaxes(title_text=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pepg data\n",
    "savetxt('data\\\\Results\\\\NN_training\\\\online_training\\\\PEPG_test_acc.csv', test_acc_PEPG, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use CMA to train the FFNN\n",
    "- This doesn't work at all this simple architecture has too many parameters ... so CMA is painfully slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Neural_Net(\n",
      "  (NN_stack): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=4, out_features=10, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "{i+1}Epoch [1/100], Step [1/1], Loss: 1.1388428211212158, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [2/100], Step [1/1], Loss: 1.1070222854614258, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [3/100], Step [1/1], Loss: 1.0813188552856445, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [4/100], Step [1/1], Loss: 0.9951866269111633, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [5/100], Step [1/1], Loss: 1.008622646331787, Test Accuracy: 31.57894736842105%\n",
      "{i+1}Epoch [6/100], Step [1/1], Loss: 0.8917995095252991, Test Accuracy: 63.1578947368421%\n",
      "{i+1}Epoch [7/100], Step [1/1], Loss: 0.9529407620429993, Test Accuracy: 60.526315789473685%\n",
      "{i+1}Epoch [8/100], Step [1/1], Loss: 0.9257021546363831, Test Accuracy: 57.89473684210526%\n",
      "{i+1}Epoch [9/100], Step [1/1], Loss: 0.842953622341156, Test Accuracy: 73.6842105263158%\n",
      "{i+1}Epoch [10/100], Step [1/1], Loss: 0.7924845814704895, Test Accuracy: 65.78947368421052%\n",
      "{i+1}Epoch [11/100], Step [1/1], Loss: 0.7407358288764954, Test Accuracy: 68.42105263157895%\n",
      "{i+1}Epoch [12/100], Step [1/1], Loss: 0.6866073608398438, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [13/100], Step [1/1], Loss: 0.6222715973854065, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [14/100], Step [1/1], Loss: 0.5103965997695923, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [15/100], Step [1/1], Loss: 0.7229110598564148, Test Accuracy: 68.42105263157895%\n",
      "{i+1}Epoch [16/100], Step [1/1], Loss: 0.6407902836799622, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [17/100], Step [1/1], Loss: 0.6052964925765991, Test Accuracy: 73.6842105263158%\n",
      "{i+1}Epoch [18/100], Step [1/1], Loss: 0.6469131708145142, Test Accuracy: 65.78947368421052%\n",
      "{i+1}Epoch [19/100], Step [1/1], Loss: 0.5170089602470398, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [20/100], Step [1/1], Loss: 0.5269883275032043, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [21/100], Step [1/1], Loss: 0.5495598316192627, Test Accuracy: 65.78947368421052%\n",
      "{i+1}Epoch [22/100], Step [1/1], Loss: 0.4593314528465271, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [23/100], Step [1/1], Loss: 0.4753897786140442, Test Accuracy: 73.6842105263158%\n",
      "{i+1}Epoch [24/100], Step [1/1], Loss: 0.39137589931488037, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [25/100], Step [1/1], Loss: 0.3920043706893921, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [26/100], Step [1/1], Loss: 0.43179312348365784, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [27/100], Step [1/1], Loss: 0.3814808130264282, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [28/100], Step [1/1], Loss: 0.3937416970729828, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [29/100], Step [1/1], Loss: 0.3628878891468048, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [30/100], Step [1/1], Loss: 0.32654517889022827, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [31/100], Step [1/1], Loss: 0.3673686981201172, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [32/100], Step [1/1], Loss: 0.4756448566913605, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [33/100], Step [1/1], Loss: 0.37147048115730286, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [34/100], Step [1/1], Loss: 0.4514821171760559, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [35/100], Step [1/1], Loss: 0.43535321950912476, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [36/100], Step [1/1], Loss: 0.49705770611763, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [37/100], Step [1/1], Loss: 0.43384456634521484, Test Accuracy: 76.3157894736842%\n",
      "{i+1}Epoch [38/100], Step [1/1], Loss: 0.44072556495666504, Test Accuracy: 76.3157894736842%\n",
      "{i+1}Epoch [39/100], Step [1/1], Loss: 0.3355587422847748, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [40/100], Step [1/1], Loss: 0.3237200677394867, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [41/100], Step [1/1], Loss: 0.3137333393096924, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [42/100], Step [1/1], Loss: 0.3429461419582367, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [43/100], Step [1/1], Loss: 0.3059251606464386, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [44/100], Step [1/1], Loss: 0.3214850127696991, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [45/100], Step [1/1], Loss: 0.3482959270477295, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [46/100], Step [1/1], Loss: 0.2877334654331207, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [47/100], Step [1/1], Loss: 0.334143728017807, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [48/100], Step [1/1], Loss: 0.3480883538722992, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [49/100], Step [1/1], Loss: 0.25276854634284973, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [50/100], Step [1/1], Loss: 0.2999033033847809, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [51/100], Step [1/1], Loss: 0.3432297110557556, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [52/100], Step [1/1], Loss: 0.2909112870693207, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [53/100], Step [1/1], Loss: 0.3573574721813202, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [54/100], Step [1/1], Loss: 0.275568425655365, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [55/100], Step [1/1], Loss: 0.3876619040966034, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [56/100], Step [1/1], Loss: 0.2635994553565979, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [57/100], Step [1/1], Loss: 0.359927237033844, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [58/100], Step [1/1], Loss: 0.2574777603149414, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [59/100], Step [1/1], Loss: 0.3194133937358856, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [60/100], Step [1/1], Loss: 0.29009753465652466, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [61/100], Step [1/1], Loss: 0.35217997431755066, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [62/100], Step [1/1], Loss: 0.22751587629318237, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [63/100], Step [1/1], Loss: 0.24001824855804443, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [64/100], Step [1/1], Loss: 0.21330256760120392, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [65/100], Step [1/1], Loss: 0.19448837637901306, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [66/100], Step [1/1], Loss: 0.25667333602905273, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [67/100], Step [1/1], Loss: 0.2588597238063812, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [68/100], Step [1/1], Loss: 0.2089201807975769, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [69/100], Step [1/1], Loss: 0.22302913665771484, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [70/100], Step [1/1], Loss: 0.20328354835510254, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [71/100], Step [1/1], Loss: 0.21277134120464325, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [72/100], Step [1/1], Loss: 0.23380862176418304, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [73/100], Step [1/1], Loss: 0.16629691421985626, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [74/100], Step [1/1], Loss: 0.28405001759529114, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [75/100], Step [1/1], Loss: 0.3234858214855194, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [76/100], Step [1/1], Loss: 0.1888951063156128, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [77/100], Step [1/1], Loss: 0.3783618211746216, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [78/100], Step [1/1], Loss: 0.29342517256736755, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [79/100], Step [1/1], Loss: 0.24502171576023102, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [80/100], Step [1/1], Loss: 0.2451215237379074, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [81/100], Step [1/1], Loss: 0.22862471640110016, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [82/100], Step [1/1], Loss: 0.18464955687522888, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [83/100], Step [1/1], Loss: 0.2365218698978424, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [84/100], Step [1/1], Loss: 0.22901204228401184, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [85/100], Step [1/1], Loss: 0.22193706035614014, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [86/100], Step [1/1], Loss: 0.1783289909362793, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [87/100], Step [1/1], Loss: 0.25464358925819397, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [88/100], Step [1/1], Loss: 0.18305695056915283, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [89/100], Step [1/1], Loss: 0.23875193297863007, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [90/100], Step [1/1], Loss: 0.20823445916175842, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [91/100], Step [1/1], Loss: 0.173030287027359, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [92/100], Step [1/1], Loss: 0.1493801474571228, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [93/100], Step [1/1], Loss: 0.1591378003358841, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [94/100], Step [1/1], Loss: 0.26223206520080566, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [95/100], Step [1/1], Loss: 0.15601123869419098, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [96/100], Step [1/1], Loss: 0.20184992253780365, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [97/100], Step [1/1], Loss: 0.2877054214477539, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [98/100], Step [1/1], Loss: 0.16522742807865143, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [99/100], Step [1/1], Loss: 0.18761934340000153, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [100/100], Step [1/1], Loss: 0.4475085735321045, Test Accuracy: 81.57894736842105%\n"
     ]
    }
   ],
   "source": [
    "#Using CMA-ES for training the NN\n",
    "n_epochs =100\n",
    "NN_IRIS = Neural_Net(input_size=4, hidden_size=10, n_classes=3)\n",
    "N_dim = NN_IRIS.count_parameters()\n",
    "pop_size = 10\n",
    "#specify we don't need the computation graph to keep track of the gradients, we will use CMAES to update the weights\n",
    "with torch.no_grad():\n",
    "    for param in NN_IRIS.parameters():\n",
    "        param.requires_grad = False\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# learning parameters\n",
    "\n",
    "init_pos = NN_IRIS.get_params()\n",
    "\n",
    "if init_pos.requires_grad:\n",
    "    # Detach the tensor from the computation graph\n",
    "    init_pos = init_pos.detach()\n",
    "if init_pos.is_cuda:\n",
    "    # Move the tensor to the CPU\n",
    "    init_pos = init_pos.cpu()\n",
    "init_pos = init_pos.numpy()\n",
    "\n",
    "CMA_optimizer = CMA_opt(N_dim, pop_size, select_pop=int(pop_size/2), sigma_init=0.1, mean_init=init_pos)\n",
    "CMA_optimizer.eigen_update_frequency = 10\n",
    "\n",
    "test_acc_CMA,best_reward_CMA = train_online_pop_NN(NN_IRIS, n_epochs, Iris_train_loader, Iris_test_loader, loss, CMA_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "PEPG",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          31.57894736842105,
          63.1578947368421,
          60.526315789473685,
          57.89473684210526,
          73.6842105263158,
          65.78947368421052,
          68.42105263157895,
          78.94736842105263,
          78.94736842105263,
          78.94736842105263,
          68.42105263157895,
          81.57894736842105,
          73.6842105263158,
          65.78947368421052,
          78.94736842105263,
          78.94736842105263,
          65.78947368421052,
          92.10526315789474,
          73.6842105263158,
          84.21052631578948,
          84.21052631578948,
          81.57894736842105,
          84.21052631578948,
          89.47368421052632,
          84.21052631578948,
          86.84210526315789,
          86.84210526315789,
          84.21052631578948,
          81.57894736842105,
          81.57894736842105,
          81.57894736842105,
          84.21052631578948,
          76.3157894736842,
          76.3157894736842,
          86.84210526315789,
          92.10526315789474,
          92.10526315789474,
          84.21052631578948,
          92.10526315789474,
          92.10526315789474,
          89.47368421052632,
          94.73684210526316,
          89.47368421052632,
          81.57894736842105,
          92.10526315789474,
          84.21052631578948,
          81.57894736842105,
          86.84210526315789,
          81.57894736842105,
          92.10526315789474,
          78.94736842105263,
          92.10526315789474,
          81.57894736842105,
          97.36842105263158,
          81.57894736842105,
          86.84210526315789,
          81.57894736842105,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          86.84210526315789,
          86.84210526315789,
          97.36842105263158,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          81.57894736842105,
          81.57894736842105,
          100,
          78.94736842105263,
          81.57894736842105,
          84.21052631578948,
          97.36842105263158,
          92.10526315789474,
          100,
          92.10526315789474,
          97.36842105263158,
          92.10526315789474,
          100,
          81.57894736842105,
          97.36842105263158,
          84.21052631578948,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          84.21052631578948,
          97.36842105263158,
          89.47368421052632,
          81.57894736842105,
          97.36842105263158,
          94.73684210526316,
          81.57894736842105
         ]
        }
       ],
       "layout": {
        "height": 300,
        "margin": {
         "b": 20,
         "l": 20,
         "r": 20,
         "t": 20
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "width": 400,
        "xaxis": {
         "title": {
          "text": "Epochs"
         },
         "type": "log"
        },
        "yaxis": {
         "title": {
          "text": "Accuracy [%]"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(test_acc_CMA)), y=test_acc_CMA, mode='lines', name='PEPG'))\n",
    "#change theme to white and set the sizer of the plot\n",
    "fig.update_layout(template='plotly_white', width=400, height=300,margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig.update_xaxes(title_text=\"Epochs\",type = 'log')\n",
    "fig.update_yaxes(title_text=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Neural_Net(\n",
      "  (NN_stack): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=4, out_features=10, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "{i+1}Epoch [1/1000], Step [1/1], Loss: 1.5929864645004272, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [2/1000], Step [1/1], Loss: 1.570778250694275, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [3/1000], Step [1/1], Loss: 1.5515416860580444, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [4/1000], Step [1/1], Loss: 1.5360982418060303, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [5/1000], Step [1/1], Loss: 1.514479637145996, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [6/1000], Step [1/1], Loss: 1.4864768981933594, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [7/1000], Step [1/1], Loss: 1.4634218215942383, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [8/1000], Step [1/1], Loss: 1.4251213073730469, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [9/1000], Step [1/1], Loss: 1.3920096158981323, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [10/1000], Step [1/1], Loss: 1.3623344898223877, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [11/1000], Step [1/1], Loss: 1.3362843990325928, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [12/1000], Step [1/1], Loss: 1.3102725744247437, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [13/1000], Step [1/1], Loss: 1.2892025709152222, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [14/1000], Step [1/1], Loss: 1.2713505029678345, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [15/1000], Step [1/1], Loss: 1.2561243772506714, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [16/1000], Step [1/1], Loss: 1.2404123544692993, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [17/1000], Step [1/1], Loss: 1.2261344194412231, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [18/1000], Step [1/1], Loss: 1.2140142917633057, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [19/1000], Step [1/1], Loss: 1.2031500339508057, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [20/1000], Step [1/1], Loss: 1.189208745956421, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [21/1000], Step [1/1], Loss: 1.1774415969848633, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [22/1000], Step [1/1], Loss: 1.1670255661010742, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [23/1000], Step [1/1], Loss: 1.1576757431030273, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [24/1000], Step [1/1], Loss: 1.1499160528182983, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [25/1000], Step [1/1], Loss: 1.1416951417922974, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [26/1000], Step [1/1], Loss: 1.134820580482483, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [27/1000], Step [1/1], Loss: 1.1275562047958374, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [28/1000], Step [1/1], Loss: 1.1194007396697998, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [29/1000], Step [1/1], Loss: 1.1151235103607178, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [30/1000], Step [1/1], Loss: 1.1126703023910522, Test Accuracy: 31.57894736842105%\n",
      "{i+1}Epoch [31/1000], Step [1/1], Loss: 1.1100008487701416, Test Accuracy: 26.31578947368421%\n",
      "{i+1}Epoch [32/1000], Step [1/1], Loss: 1.1087735891342163, Test Accuracy: 28.94736842105263%\n",
      "{i+1}Epoch [33/1000], Step [1/1], Loss: 1.10749089717865, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [34/1000], Step [1/1], Loss: 1.1069271564483643, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [35/1000], Step [1/1], Loss: 1.1068567037582397, Test Accuracy: 26.31578947368421%\n",
      "{i+1}Epoch [36/1000], Step [1/1], Loss: 1.1072975397109985, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [37/1000], Step [1/1], Loss: 1.1084054708480835, Test Accuracy: 42.10526315789474%\n",
      "{i+1}Epoch [38/1000], Step [1/1], Loss: 1.110086441040039, Test Accuracy: 42.10526315789474%\n",
      "{i+1}Epoch [39/1000], Step [1/1], Loss: 1.1122639179229736, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [40/1000], Step [1/1], Loss: 1.1140598058700562, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [41/1000], Step [1/1], Loss: 1.1143231391906738, Test Accuracy: 28.94736842105263%\n",
      "{i+1}Epoch [42/1000], Step [1/1], Loss: 1.114888072013855, Test Accuracy: 28.94736842105263%\n",
      "{i+1}Epoch [43/1000], Step [1/1], Loss: 1.1145048141479492, Test Accuracy: 26.31578947368421%\n",
      "{i+1}Epoch [44/1000], Step [1/1], Loss: 1.1144933700561523, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [45/1000], Step [1/1], Loss: 1.1135833263397217, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [46/1000], Step [1/1], Loss: 1.1128226518630981, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [47/1000], Step [1/1], Loss: 1.1113039255142212, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [48/1000], Step [1/1], Loss: 1.1100263595581055, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [49/1000], Step [1/1], Loss: 1.1085842847824097, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [50/1000], Step [1/1], Loss: 1.107325553894043, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [51/1000], Step [1/1], Loss: 1.1064766645431519, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [52/1000], Step [1/1], Loss: 1.1056056022644043, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [53/1000], Step [1/1], Loss: 1.1024051904678345, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [54/1000], Step [1/1], Loss: 1.0969184637069702, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [55/1000], Step [1/1], Loss: 1.0879379510879517, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [56/1000], Step [1/1], Loss: 1.079817771911621, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [57/1000], Step [1/1], Loss: 1.0694527626037598, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [58/1000], Step [1/1], Loss: 1.0581451654434204, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [59/1000], Step [1/1], Loss: 1.0473729372024536, Test Accuracy: 23.68421052631579%\n",
      "{i+1}Epoch [60/1000], Step [1/1], Loss: 1.0381383895874023, Test Accuracy: 26.31578947368421%\n",
      "{i+1}Epoch [61/1000], Step [1/1], Loss: 1.029375672340393, Test Accuracy: 26.31578947368421%\n",
      "{i+1}Epoch [62/1000], Step [1/1], Loss: 1.0206749439239502, Test Accuracy: 42.10526315789474%\n",
      "{i+1}Epoch [63/1000], Step [1/1], Loss: 1.0117193460464478, Test Accuracy: 42.10526315789474%\n",
      "{i+1}Epoch [64/1000], Step [1/1], Loss: 1.0017975568771362, Test Accuracy: 50.0%\n",
      "{i+1}Epoch [65/1000], Step [1/1], Loss: 0.9928358793258667, Test Accuracy: 52.63157894736842%\n",
      "{i+1}Epoch [66/1000], Step [1/1], Loss: 0.984444260597229, Test Accuracy: 50.0%\n",
      "{i+1}Epoch [67/1000], Step [1/1], Loss: 0.9763092994689941, Test Accuracy: 50.0%\n",
      "{i+1}Epoch [68/1000], Step [1/1], Loss: 0.9689119458198547, Test Accuracy: 55.26315789473684%\n",
      "{i+1}Epoch [69/1000], Step [1/1], Loss: 0.9603450894355774, Test Accuracy: 68.42105263157895%\n",
      "{i+1}Epoch [70/1000], Step [1/1], Loss: 0.9525688290596008, Test Accuracy: 71.05263157894737%\n",
      "{i+1}Epoch [71/1000], Step [1/1], Loss: 0.9454551339149475, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [72/1000], Step [1/1], Loss: 0.9390752911567688, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [73/1000], Step [1/1], Loss: 0.9324212074279785, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [74/1000], Step [1/1], Loss: 0.9264453053474426, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [75/1000], Step [1/1], Loss: 0.9213557839393616, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [76/1000], Step [1/1], Loss: 0.9169418215751648, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [77/1000], Step [1/1], Loss: 0.9111508727073669, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [78/1000], Step [1/1], Loss: 0.9059678912162781, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [79/1000], Step [1/1], Loss: 0.9012721180915833, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [80/1000], Step [1/1], Loss: 0.8971149325370789, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [81/1000], Step [1/1], Loss: 0.8933123350143433, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [82/1000], Step [1/1], Loss: 0.8899408578872681, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [83/1000], Step [1/1], Loss: 0.8868429064750671, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [84/1000], Step [1/1], Loss: 0.8840836882591248, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [85/1000], Step [1/1], Loss: 0.8816434144973755, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [86/1000], Step [1/1], Loss: 0.879555881023407, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [87/1000], Step [1/1], Loss: 0.8778711557388306, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [88/1000], Step [1/1], Loss: 0.8762110471725464, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [89/1000], Step [1/1], Loss: 0.8749065399169922, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [90/1000], Step [1/1], Loss: 0.8734647631645203, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [91/1000], Step [1/1], Loss: 0.8723329305648804, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [92/1000], Step [1/1], Loss: 0.871130108833313, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [93/1000], Step [1/1], Loss: 0.8703133463859558, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [94/1000], Step [1/1], Loss: 0.8672052621841431, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [95/1000], Step [1/1], Loss: 0.8645299673080444, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [96/1000], Step [1/1], Loss: 0.8614460825920105, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [97/1000], Step [1/1], Loss: 0.8586699962615967, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [98/1000], Step [1/1], Loss: 0.8559464812278748, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [99/1000], Step [1/1], Loss: 0.8512094616889954, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [100/1000], Step [1/1], Loss: 0.8470103740692139, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [101/1000], Step [1/1], Loss: 0.8403967618942261, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [102/1000], Step [1/1], Loss: 0.8343653082847595, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [103/1000], Step [1/1], Loss: 0.8288450241088867, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [104/1000], Step [1/1], Loss: 0.8238925337791443, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [105/1000], Step [1/1], Loss: 0.819282591342926, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [106/1000], Step [1/1], Loss: 0.8150157928466797, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [107/1000], Step [1/1], Loss: 0.8110913634300232, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [108/1000], Step [1/1], Loss: 0.8076302409172058, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [109/1000], Step [1/1], Loss: 0.8047828674316406, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [110/1000], Step [1/1], Loss: 0.8016890287399292, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [111/1000], Step [1/1], Loss: 0.7985843420028687, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [112/1000], Step [1/1], Loss: 0.7960419654846191, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [113/1000], Step [1/1], Loss: 0.7938725352287292, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [114/1000], Step [1/1], Loss: 0.7919220924377441, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [115/1000], Step [1/1], Loss: 0.7881788015365601, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [116/1000], Step [1/1], Loss: 0.785388171672821, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [117/1000], Step [1/1], Loss: 0.782651424407959, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [118/1000], Step [1/1], Loss: 0.7809545397758484, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [119/1000], Step [1/1], Loss: 0.7796934843063354, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [120/1000], Step [1/1], Loss: 0.7779404520988464, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [121/1000], Step [1/1], Loss: 0.7758587598800659, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [122/1000], Step [1/1], Loss: 0.7741915583610535, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [123/1000], Step [1/1], Loss: 0.7728286981582642, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [124/1000], Step [1/1], Loss: 0.7714756727218628, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [125/1000], Step [1/1], Loss: 0.7703604698181152, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [126/1000], Step [1/1], Loss: 0.7686046957969666, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [127/1000], Step [1/1], Loss: 0.7671717405319214, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [128/1000], Step [1/1], Loss: 0.7647652626037598, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [129/1000], Step [1/1], Loss: 0.7617877721786499, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [130/1000], Step [1/1], Loss: 0.7593093514442444, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [131/1000], Step [1/1], Loss: 0.7517517805099487, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [132/1000], Step [1/1], Loss: 0.7422716021537781, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [133/1000], Step [1/1], Loss: 0.7338076233863831, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [134/1000], Step [1/1], Loss: 0.726186215877533, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [135/1000], Step [1/1], Loss: 0.7188730239868164, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [136/1000], Step [1/1], Loss: 0.7121533751487732, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [137/1000], Step [1/1], Loss: 0.706384003162384, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [138/1000], Step [1/1], Loss: 0.7012887001037598, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [139/1000], Step [1/1], Loss: 0.6966242790222168, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [140/1000], Step [1/1], Loss: 0.6911781430244446, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [141/1000], Step [1/1], Loss: 0.6860204339027405, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [142/1000], Step [1/1], Loss: 0.6815750002861023, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [143/1000], Step [1/1], Loss: 0.6771451830863953, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [144/1000], Step [1/1], Loss: 0.673050045967102, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [145/1000], Step [1/1], Loss: 0.6694605946540833, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [146/1000], Step [1/1], Loss: 0.666485607624054, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [147/1000], Step [1/1], Loss: 0.6639233231544495, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [148/1000], Step [1/1], Loss: 0.663021445274353, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [149/1000], Step [1/1], Loss: 0.6624252200126648, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [150/1000], Step [1/1], Loss: 0.6628161072731018, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [151/1000], Step [1/1], Loss: 0.6632134318351746, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [152/1000], Step [1/1], Loss: 0.6640117764472961, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [153/1000], Step [1/1], Loss: 0.6636176109313965, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [154/1000], Step [1/1], Loss: 0.6619938611984253, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [155/1000], Step [1/1], Loss: 0.6593505144119263, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [156/1000], Step [1/1], Loss: 0.6561071276664734, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [157/1000], Step [1/1], Loss: 0.6527110934257507, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [158/1000], Step [1/1], Loss: 0.6485804915428162, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [159/1000], Step [1/1], Loss: 0.6449814438819885, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [160/1000], Step [1/1], Loss: 0.6418201327323914, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [161/1000], Step [1/1], Loss: 0.6386397480964661, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [162/1000], Step [1/1], Loss: 0.635434627532959, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [163/1000], Step [1/1], Loss: 0.6326228380203247, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [164/1000], Step [1/1], Loss: 0.6301862597465515, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [165/1000], Step [1/1], Loss: 0.6263890862464905, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [166/1000], Step [1/1], Loss: 0.622656524181366, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [167/1000], Step [1/1], Loss: 0.6178746223449707, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [168/1000], Step [1/1], Loss: 0.6135753989219666, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [169/1000], Step [1/1], Loss: 0.6099900603294373, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [170/1000], Step [1/1], Loss: 0.6053597331047058, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [171/1000], Step [1/1], Loss: 0.6014288663864136, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [172/1000], Step [1/1], Loss: 0.5978941917419434, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [173/1000], Step [1/1], Loss: 0.5941591262817383, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [174/1000], Step [1/1], Loss: 0.5908201336860657, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [175/1000], Step [1/1], Loss: 0.5869329571723938, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [176/1000], Step [1/1], Loss: 0.5833521485328674, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [177/1000], Step [1/1], Loss: 0.5801268815994263, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [178/1000], Step [1/1], Loss: 0.5772160887718201, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [179/1000], Step [1/1], Loss: 0.5738776922225952, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [180/1000], Step [1/1], Loss: 0.57062828540802, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [181/1000], Step [1/1], Loss: 0.5669511556625366, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [182/1000], Step [1/1], Loss: 0.5637443661689758, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [183/1000], Step [1/1], Loss: 0.5608283877372742, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [184/1000], Step [1/1], Loss: 0.5579640865325928, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [185/1000], Step [1/1], Loss: 0.5546992421150208, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [186/1000], Step [1/1], Loss: 0.5519761443138123, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [187/1000], Step [1/1], Loss: 0.5499860644340515, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [188/1000], Step [1/1], Loss: 0.5482984185218811, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [189/1000], Step [1/1], Loss: 0.547169029712677, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [190/1000], Step [1/1], Loss: 0.5455896258354187, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [191/1000], Step [1/1], Loss: 0.5447545051574707, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [192/1000], Step [1/1], Loss: 0.5443801879882812, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [193/1000], Step [1/1], Loss: 0.5444132685661316, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [194/1000], Step [1/1], Loss: 0.5425614714622498, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [195/1000], Step [1/1], Loss: 0.5408094525337219, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [196/1000], Step [1/1], Loss: 0.5396880507469177, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [197/1000], Step [1/1], Loss: 0.5395175814628601, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [198/1000], Step [1/1], Loss: 0.5396741032600403, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [199/1000], Step [1/1], Loss: 0.5381509065628052, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [200/1000], Step [1/1], Loss: 0.5365636348724365, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [201/1000], Step [1/1], Loss: 0.5347450375556946, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [202/1000], Step [1/1], Loss: 0.5316814184188843, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [203/1000], Step [1/1], Loss: 0.5289201736450195, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [204/1000], Step [1/1], Loss: 0.5255405306816101, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [205/1000], Step [1/1], Loss: 0.5219553112983704, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [206/1000], Step [1/1], Loss: 0.5175786018371582, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [207/1000], Step [1/1], Loss: 0.5128943920135498, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [208/1000], Step [1/1], Loss: 0.5087355971336365, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [209/1000], Step [1/1], Loss: 0.5050904154777527, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [210/1000], Step [1/1], Loss: 0.5021278858184814, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [211/1000], Step [1/1], Loss: 0.49945876002311707, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [212/1000], Step [1/1], Loss: 0.49731069803237915, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [213/1000], Step [1/1], Loss: 0.4948156177997589, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [214/1000], Step [1/1], Loss: 0.49217626452445984, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [215/1000], Step [1/1], Loss: 0.48981577157974243, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [216/1000], Step [1/1], Loss: 0.4863848388195038, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [217/1000], Step [1/1], Loss: 0.48379477858543396, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [218/1000], Step [1/1], Loss: 0.4822232723236084, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [219/1000], Step [1/1], Loss: 0.47905978560447693, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [220/1000], Step [1/1], Loss: 0.47526419162750244, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [221/1000], Step [1/1], Loss: 0.4718655049800873, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [222/1000], Step [1/1], Loss: 0.46755605936050415, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [223/1000], Step [1/1], Loss: 0.46332669258117676, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [224/1000], Step [1/1], Loss: 0.45928955078125, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [225/1000], Step [1/1], Loss: 0.4557446837425232, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [226/1000], Step [1/1], Loss: 0.45247721672058105, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [227/1000], Step [1/1], Loss: 0.44982558488845825, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [228/1000], Step [1/1], Loss: 0.44762662053108215, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [229/1000], Step [1/1], Loss: 0.4456774592399597, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [230/1000], Step [1/1], Loss: 0.4438048303127289, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [231/1000], Step [1/1], Loss: 0.44267910718917847, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [232/1000], Step [1/1], Loss: 0.44200003147125244, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [233/1000], Step [1/1], Loss: 0.4418196380138397, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [234/1000], Step [1/1], Loss: 0.4435231685638428, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [235/1000], Step [1/1], Loss: 0.44518911838531494, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [236/1000], Step [1/1], Loss: 0.44649240374565125, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [237/1000], Step [1/1], Loss: 0.4471956491470337, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [238/1000], Step [1/1], Loss: 0.44791683554649353, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [239/1000], Step [1/1], Loss: 0.4485209584236145, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [240/1000], Step [1/1], Loss: 0.4491070806980133, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [241/1000], Step [1/1], Loss: 0.44966989755630493, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [242/1000], Step [1/1], Loss: 0.45061933994293213, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [243/1000], Step [1/1], Loss: 0.4517801105976105, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [244/1000], Step [1/1], Loss: 0.4528748393058777, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [245/1000], Step [1/1], Loss: 0.4540237486362457, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [246/1000], Step [1/1], Loss: 0.45296168327331543, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [247/1000], Step [1/1], Loss: 0.4518021047115326, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [248/1000], Step [1/1], Loss: 0.4500693678855896, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [249/1000], Step [1/1], Loss: 0.44825971126556396, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [250/1000], Step [1/1], Loss: 0.4467311203479767, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [251/1000], Step [1/1], Loss: 0.4452650249004364, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [252/1000], Step [1/1], Loss: 0.44401654601097107, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [253/1000], Step [1/1], Loss: 0.4408900737762451, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [254/1000], Step [1/1], Loss: 0.43906718492507935, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [255/1000], Step [1/1], Loss: 0.43753954768180847, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [256/1000], Step [1/1], Loss: 0.43627452850341797, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [257/1000], Step [1/1], Loss: 0.43553757667541504, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [258/1000], Step [1/1], Loss: 0.4348601698875427, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [259/1000], Step [1/1], Loss: 0.4346752166748047, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [260/1000], Step [1/1], Loss: 0.43498125672340393, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [261/1000], Step [1/1], Loss: 0.4353548288345337, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [262/1000], Step [1/1], Loss: 0.43472525477409363, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [263/1000], Step [1/1], Loss: 0.4306909143924713, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [264/1000], Step [1/1], Loss: 0.42708757519721985, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [265/1000], Step [1/1], Loss: 0.4229896366596222, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [266/1000], Step [1/1], Loss: 0.4194280207157135, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [267/1000], Step [1/1], Loss: 0.41644373536109924, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [268/1000], Step [1/1], Loss: 0.41511020064353943, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [269/1000], Step [1/1], Loss: 0.41373249888420105, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [270/1000], Step [1/1], Loss: 0.41247788071632385, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [271/1000], Step [1/1], Loss: 0.4119149148464203, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [272/1000], Step [1/1], Loss: 0.41068965196609497, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [273/1000], Step [1/1], Loss: 0.4086243808269501, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [274/1000], Step [1/1], Loss: 0.40741682052612305, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [275/1000], Step [1/1], Loss: 0.4063653349876404, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [276/1000], Step [1/1], Loss: 0.4050833284854889, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [277/1000], Step [1/1], Loss: 0.4043470323085785, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [278/1000], Step [1/1], Loss: 0.4037325978279114, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [279/1000], Step [1/1], Loss: 0.4022325873374939, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [280/1000], Step [1/1], Loss: 0.4008288085460663, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [281/1000], Step [1/1], Loss: 0.3983019292354584, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [282/1000], Step [1/1], Loss: 0.3963463306427002, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [283/1000], Step [1/1], Loss: 0.3946880102157593, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [284/1000], Step [1/1], Loss: 0.39330634474754333, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [285/1000], Step [1/1], Loss: 0.3921589255332947, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [286/1000], Step [1/1], Loss: 0.3912082314491272, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [287/1000], Step [1/1], Loss: 0.39084598422050476, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [288/1000], Step [1/1], Loss: 0.39206239581108093, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [289/1000], Step [1/1], Loss: 0.39217209815979004, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [290/1000], Step [1/1], Loss: 0.39155834913253784, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [291/1000], Step [1/1], Loss: 0.3901333808898926, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [292/1000], Step [1/1], Loss: 0.38874462246894836, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [293/1000], Step [1/1], Loss: 0.38805335760116577, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [294/1000], Step [1/1], Loss: 0.38735508918762207, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [295/1000], Step [1/1], Loss: 0.386740118265152, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [296/1000], Step [1/1], Loss: 0.38514137268066406, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [297/1000], Step [1/1], Loss: 0.38420042395591736, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [298/1000], Step [1/1], Loss: 0.38348984718322754, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [299/1000], Step [1/1], Loss: 0.3829159140586853, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [300/1000], Step [1/1], Loss: 0.3817031681537628, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [301/1000], Step [1/1], Loss: 0.3811214566230774, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [302/1000], Step [1/1], Loss: 0.38070860505104065, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [303/1000], Step [1/1], Loss: 0.38078147172927856, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [304/1000], Step [1/1], Loss: 0.3808939754962921, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [305/1000], Step [1/1], Loss: 0.3806723654270172, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [306/1000], Step [1/1], Loss: 0.380507230758667, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [307/1000], Step [1/1], Loss: 0.37942975759506226, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [308/1000], Step [1/1], Loss: 0.3764299154281616, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [309/1000], Step [1/1], Loss: 0.3721495270729065, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [310/1000], Step [1/1], Loss: 0.3687773644924164, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [311/1000], Step [1/1], Loss: 0.3655172288417816, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [312/1000], Step [1/1], Loss: 0.36282575130462646, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [313/1000], Step [1/1], Loss: 0.36012521386146545, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [314/1000], Step [1/1], Loss: 0.3579903244972229, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [315/1000], Step [1/1], Loss: 0.35561051964759827, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [316/1000], Step [1/1], Loss: 0.3532272279262543, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [317/1000], Step [1/1], Loss: 0.3541054129600525, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [318/1000], Step [1/1], Loss: 0.35579341650009155, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [319/1000], Step [1/1], Loss: 0.357343852519989, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [320/1000], Step [1/1], Loss: 0.3586359918117523, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [321/1000], Step [1/1], Loss: 0.36031121015548706, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [322/1000], Step [1/1], Loss: 0.3615792989730835, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [323/1000], Step [1/1], Loss: 0.3608928620815277, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [324/1000], Step [1/1], Loss: 0.35789260268211365, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [325/1000], Step [1/1], Loss: 0.35504013299942017, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [326/1000], Step [1/1], Loss: 0.3525131344795227, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [327/1000], Step [1/1], Loss: 0.3502480685710907, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [328/1000], Step [1/1], Loss: 0.3486320972442627, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [329/1000], Step [1/1], Loss: 0.3475705087184906, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [330/1000], Step [1/1], Loss: 0.34725290536880493, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [331/1000], Step [1/1], Loss: 0.346769243478775, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [332/1000], Step [1/1], Loss: 0.3460093140602112, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [333/1000], Step [1/1], Loss: 0.3452160954475403, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [334/1000], Step [1/1], Loss: 0.344015508890152, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [335/1000], Step [1/1], Loss: 0.34206098318099976, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [336/1000], Step [1/1], Loss: 0.3403603732585907, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [337/1000], Step [1/1], Loss: 0.33934974670410156, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [338/1000], Step [1/1], Loss: 0.3382132053375244, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [339/1000], Step [1/1], Loss: 0.3379918932914734, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [340/1000], Step [1/1], Loss: 0.3377092480659485, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [341/1000], Step [1/1], Loss: 0.3378560245037079, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [342/1000], Step [1/1], Loss: 0.3384767770767212, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [343/1000], Step [1/1], Loss: 0.33921417593955994, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [344/1000], Step [1/1], Loss: 0.3403947353363037, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [345/1000], Step [1/1], Loss: 0.3407120108604431, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [346/1000], Step [1/1], Loss: 0.3418004512786865, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [347/1000], Step [1/1], Loss: 0.3408797085285187, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [348/1000], Step [1/1], Loss: 0.3365679085254669, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [349/1000], Step [1/1], Loss: 0.33339864015579224, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [350/1000], Step [1/1], Loss: 0.3308800458908081, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [351/1000], Step [1/1], Loss: 0.32908934354782104, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [352/1000], Step [1/1], Loss: 0.32955336570739746, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [353/1000], Step [1/1], Loss: 0.3300144672393799, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [354/1000], Step [1/1], Loss: 0.3300895690917969, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [355/1000], Step [1/1], Loss: 0.33021652698516846, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [356/1000], Step [1/1], Loss: 0.3299778997898102, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [357/1000], Step [1/1], Loss: 0.32828769087791443, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [358/1000], Step [1/1], Loss: 0.32601457834243774, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [359/1000], Step [1/1], Loss: 0.3233857750892639, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [360/1000], Step [1/1], Loss: 0.3209948241710663, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [361/1000], Step [1/1], Loss: 0.31925609707832336, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [362/1000], Step [1/1], Loss: 0.3177114427089691, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [363/1000], Step [1/1], Loss: 0.3162529468536377, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [364/1000], Step [1/1], Loss: 0.31501054763793945, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [365/1000], Step [1/1], Loss: 0.3139984905719757, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [366/1000], Step [1/1], Loss: 0.31297817826271057, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [367/1000], Step [1/1], Loss: 0.313720703125, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [368/1000], Step [1/1], Loss: 0.3153536319732666, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [369/1000], Step [1/1], Loss: 0.3184523284435272, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [370/1000], Step [1/1], Loss: 0.32140645384788513, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [371/1000], Step [1/1], Loss: 0.32424965500831604, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [372/1000], Step [1/1], Loss: 0.3252917528152466, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [373/1000], Step [1/1], Loss: 0.32617655396461487, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [374/1000], Step [1/1], Loss: 0.32344794273376465, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [375/1000], Step [1/1], Loss: 0.321016788482666, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [376/1000], Step [1/1], Loss: 0.31884995102882385, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [377/1000], Step [1/1], Loss: 0.31760311126708984, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [378/1000], Step [1/1], Loss: 0.31639280915260315, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [379/1000], Step [1/1], Loss: 0.3133317530155182, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [380/1000], Step [1/1], Loss: 0.31064489483833313, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [381/1000], Step [1/1], Loss: 0.30857720971107483, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [382/1000], Step [1/1], Loss: 0.30865421891212463, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [383/1000], Step [1/1], Loss: 0.3086933195590973, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [384/1000], Step [1/1], Loss: 0.30922573804855347, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [385/1000], Step [1/1], Loss: 0.30987662076950073, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [386/1000], Step [1/1], Loss: 0.30910804867744446, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [387/1000], Step [1/1], Loss: 0.30870482325553894, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [388/1000], Step [1/1], Loss: 0.30789873003959656, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [389/1000], Step [1/1], Loss: 0.3071589469909668, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [390/1000], Step [1/1], Loss: 0.3072833716869354, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [391/1000], Step [1/1], Loss: 0.3077463209629059, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [392/1000], Step [1/1], Loss: 0.3078306317329407, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [393/1000], Step [1/1], Loss: 0.30784136056900024, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [394/1000], Step [1/1], Loss: 0.30762094259262085, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [395/1000], Step [1/1], Loss: 0.3063425123691559, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [396/1000], Step [1/1], Loss: 0.30541524291038513, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [397/1000], Step [1/1], Loss: 0.3044755458831787, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [398/1000], Step [1/1], Loss: 0.30323660373687744, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [399/1000], Step [1/1], Loss: 0.30220675468444824, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [400/1000], Step [1/1], Loss: 0.3003319799900055, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [401/1000], Step [1/1], Loss: 0.3006996512413025, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [402/1000], Step [1/1], Loss: 0.2994525134563446, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [403/1000], Step [1/1], Loss: 0.30010128021240234, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [404/1000], Step [1/1], Loss: 0.3006724417209625, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [405/1000], Step [1/1], Loss: 0.30194786190986633, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [406/1000], Step [1/1], Loss: 0.30288487672805786, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [407/1000], Step [1/1], Loss: 0.3028719425201416, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [408/1000], Step [1/1], Loss: 0.3029872179031372, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [409/1000], Step [1/1], Loss: 0.3024616539478302, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [410/1000], Step [1/1], Loss: 0.3020090162754059, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [411/1000], Step [1/1], Loss: 0.29950180649757385, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [412/1000], Step [1/1], Loss: 0.29728075861930847, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [413/1000], Step [1/1], Loss: 0.29533323645591736, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [414/1000], Step [1/1], Loss: 0.2968612015247345, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [415/1000], Step [1/1], Loss: 0.29840871691703796, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [416/1000], Step [1/1], Loss: 0.29951104521751404, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [417/1000], Step [1/1], Loss: 0.3015108108520508, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [418/1000], Step [1/1], Loss: 0.29982784390449524, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [419/1000], Step [1/1], Loss: 0.29821011424064636, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [420/1000], Step [1/1], Loss: 0.29579290747642517, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [421/1000], Step [1/1], Loss: 0.29357147216796875, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [422/1000], Step [1/1], Loss: 0.29008617997169495, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [423/1000], Step [1/1], Loss: 0.2868812680244446, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [424/1000], Step [1/1], Loss: 0.28234580159187317, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [425/1000], Step [1/1], Loss: 0.2785170376300812, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [426/1000], Step [1/1], Loss: 0.2747238278388977, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [427/1000], Step [1/1], Loss: 0.2722138464450836, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [428/1000], Step [1/1], Loss: 0.27032339572906494, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [429/1000], Step [1/1], Loss: 0.26821571588516235, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [430/1000], Step [1/1], Loss: 0.2674117982387543, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [431/1000], Step [1/1], Loss: 0.26745763421058655, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [432/1000], Step [1/1], Loss: 0.2680250406265259, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [433/1000], Step [1/1], Loss: 0.26924097537994385, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [434/1000], Step [1/1], Loss: 0.27014556527137756, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [435/1000], Step [1/1], Loss: 0.27235499024391174, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [436/1000], Step [1/1], Loss: 0.2745169997215271, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [437/1000], Step [1/1], Loss: 0.2772561013698578, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [438/1000], Step [1/1], Loss: 0.2798779606819153, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [439/1000], Step [1/1], Loss: 0.28130990266799927, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [440/1000], Step [1/1], Loss: 0.2778065502643585, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [441/1000], Step [1/1], Loss: 0.2747621536254883, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [442/1000], Step [1/1], Loss: 0.27222272753715515, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [443/1000], Step [1/1], Loss: 0.2708757221698761, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [444/1000], Step [1/1], Loss: 0.26948803663253784, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [445/1000], Step [1/1], Loss: 0.26711776852607727, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [446/1000], Step [1/1], Loss: 0.26495590806007385, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [447/1000], Step [1/1], Loss: 0.2630179822444916, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [448/1000], Step [1/1], Loss: 0.2609094977378845, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [449/1000], Step [1/1], Loss: 0.25926291942596436, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [450/1000], Step [1/1], Loss: 0.2574720084667206, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [451/1000], Step [1/1], Loss: 0.2563636898994446, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [452/1000], Step [1/1], Loss: 0.2554565370082855, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [453/1000], Step [1/1], Loss: 0.25433871150016785, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [454/1000], Step [1/1], Loss: 0.2559768259525299, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [455/1000], Step [1/1], Loss: 0.2576231062412262, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [456/1000], Step [1/1], Loss: 0.26044321060180664, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [457/1000], Step [1/1], Loss: 0.2629892826080322, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [458/1000], Step [1/1], Loss: 0.26617947220802307, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [459/1000], Step [1/1], Loss: 0.2652515172958374, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [460/1000], Step [1/1], Loss: 0.2633909285068512, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [461/1000], Step [1/1], Loss: 0.2616730034351349, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [462/1000], Step [1/1], Loss: 0.26120051741600037, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [463/1000], Step [1/1], Loss: 0.26008400321006775, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [464/1000], Step [1/1], Loss: 0.2590588927268982, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [465/1000], Step [1/1], Loss: 0.25806403160095215, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [466/1000], Step [1/1], Loss: 0.25723057985305786, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [467/1000], Step [1/1], Loss: 0.2558774948120117, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [468/1000], Step [1/1], Loss: 0.2528489828109741, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [469/1000], Step [1/1], Loss: 0.24993716180324554, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [470/1000], Step [1/1], Loss: 0.24766992032527924, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [471/1000], Step [1/1], Loss: 0.24586863815784454, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [472/1000], Step [1/1], Loss: 0.24435141682624817, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [473/1000], Step [1/1], Loss: 0.24323177337646484, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [474/1000], Step [1/1], Loss: 0.2457493245601654, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [475/1000], Step [1/1], Loss: 0.24865803122520447, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [476/1000], Step [1/1], Loss: 0.25073155760765076, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [477/1000], Step [1/1], Loss: 0.25289374589920044, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [478/1000], Step [1/1], Loss: 0.25432059168815613, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [479/1000], Step [1/1], Loss: 0.2554255425930023, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [480/1000], Step [1/1], Loss: 0.25645312666893005, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [481/1000], Step [1/1], Loss: 0.25658977031707764, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [482/1000], Step [1/1], Loss: 0.25626692175865173, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [483/1000], Step [1/1], Loss: 0.24801412224769592, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [484/1000], Step [1/1], Loss: 0.24170474708080292, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [485/1000], Step [1/1], Loss: 0.23608696460723877, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [486/1000], Step [1/1], Loss: 0.23333941400051117, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [487/1000], Step [1/1], Loss: 0.2323705554008484, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [488/1000], Step [1/1], Loss: 0.23164044320583344, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [489/1000], Step [1/1], Loss: 0.2308809608221054, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [490/1000], Step [1/1], Loss: 0.2304549515247345, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [491/1000], Step [1/1], Loss: 0.23135754466056824, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [492/1000], Step [1/1], Loss: 0.2337379902601242, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [493/1000], Step [1/1], Loss: 0.2365664392709732, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [494/1000], Step [1/1], Loss: 0.23923686146736145, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [495/1000], Step [1/1], Loss: 0.24159471690654755, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [496/1000], Step [1/1], Loss: 0.2438470721244812, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [497/1000], Step [1/1], Loss: 0.24588552117347717, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [498/1000], Step [1/1], Loss: 0.24771153926849365, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [499/1000], Step [1/1], Loss: 0.24891358613967896, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [500/1000], Step [1/1], Loss: 0.2489224672317505, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [501/1000], Step [1/1], Loss: 0.24386471509933472, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [502/1000], Step [1/1], Loss: 0.23931089043617249, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [503/1000], Step [1/1], Loss: 0.23533260822296143, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [504/1000], Step [1/1], Loss: 0.23229177296161652, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [505/1000], Step [1/1], Loss: 0.23064462840557098, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [506/1000], Step [1/1], Loss: 0.23027324676513672, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [507/1000], Step [1/1], Loss: 0.22980275750160217, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [508/1000], Step [1/1], Loss: 0.23035888373851776, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [509/1000], Step [1/1], Loss: 0.23127427697181702, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [510/1000], Step [1/1], Loss: 0.23278923332691193, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [511/1000], Step [1/1], Loss: 0.23623599112033844, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [512/1000], Step [1/1], Loss: 0.24000707268714905, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [513/1000], Step [1/1], Loss: 0.24393495917320251, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [514/1000], Step [1/1], Loss: 0.24775291979312897, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [515/1000], Step [1/1], Loss: 0.251187264919281, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [516/1000], Step [1/1], Loss: 0.2526363730430603, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [517/1000], Step [1/1], Loss: 0.2539752721786499, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [518/1000], Step [1/1], Loss: 0.2542705833911896, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [519/1000], Step [1/1], Loss: 0.25430288910865784, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [520/1000], Step [1/1], Loss: 0.2545498311519623, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [521/1000], Step [1/1], Loss: 0.2540236711502075, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [522/1000], Step [1/1], Loss: 0.2541032135486603, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [523/1000], Step [1/1], Loss: 0.254605770111084, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [524/1000], Step [1/1], Loss: 0.25331950187683105, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [525/1000], Step [1/1], Loss: 0.24995540082454681, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [526/1000], Step [1/1], Loss: 0.2444116473197937, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [527/1000], Step [1/1], Loss: 0.23967783153057098, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [528/1000], Step [1/1], Loss: 0.23553991317749023, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [529/1000], Step [1/1], Loss: 0.23185303807258606, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [530/1000], Step [1/1], Loss: 0.2281082272529602, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [531/1000], Step [1/1], Loss: 0.22478997707366943, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [532/1000], Step [1/1], Loss: 0.2235698252916336, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [533/1000], Step [1/1], Loss: 0.220903679728508, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [534/1000], Step [1/1], Loss: 0.22070980072021484, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [535/1000], Step [1/1], Loss: 0.22274509072303772, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [536/1000], Step [1/1], Loss: 0.22498798370361328, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [537/1000], Step [1/1], Loss: 0.2243279665708542, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [538/1000], Step [1/1], Loss: 0.22359183430671692, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [539/1000], Step [1/1], Loss: 0.22344982624053955, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [540/1000], Step [1/1], Loss: 0.2238185554742813, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [541/1000], Step [1/1], Loss: 0.22382788360118866, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [542/1000], Step [1/1], Loss: 0.2224576324224472, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [543/1000], Step [1/1], Loss: 0.22143326699733734, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [544/1000], Step [1/1], Loss: 0.2202697992324829, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [545/1000], Step [1/1], Loss: 0.2164095789194107, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [546/1000], Step [1/1], Loss: 0.21327008306980133, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [547/1000], Step [1/1], Loss: 0.21064846217632294, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [548/1000], Step [1/1], Loss: 0.20773573219776154, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [549/1000], Step [1/1], Loss: 0.2092355340719223, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [550/1000], Step [1/1], Loss: 0.21061980724334717, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [551/1000], Step [1/1], Loss: 0.20769013464450836, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [552/1000], Step [1/1], Loss: 0.20498132705688477, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [553/1000], Step [1/1], Loss: 0.20223328471183777, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [554/1000], Step [1/1], Loss: 0.19980765879154205, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [555/1000], Step [1/1], Loss: 0.19925729930400848, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [556/1000], Step [1/1], Loss: 0.20687226951122284, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [557/1000], Step [1/1], Loss: 0.21293160319328308, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [558/1000], Step [1/1], Loss: 0.21721802651882172, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [559/1000], Step [1/1], Loss: 0.2210143506526947, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [560/1000], Step [1/1], Loss: 0.2242702692747116, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [561/1000], Step [1/1], Loss: 0.22754473984241486, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [562/1000], Step [1/1], Loss: 0.2304317057132721, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [563/1000], Step [1/1], Loss: 0.22412677109241486, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [564/1000], Step [1/1], Loss: 0.2107214629650116, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [565/1000], Step [1/1], Loss: 0.19826065003871918, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [566/1000], Step [1/1], Loss: 0.18956975638866425, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [567/1000], Step [1/1], Loss: 0.18388180434703827, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [568/1000], Step [1/1], Loss: 0.18213577568531036, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [569/1000], Step [1/1], Loss: 0.1826484501361847, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [570/1000], Step [1/1], Loss: 0.18837685883045197, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [571/1000], Step [1/1], Loss: 0.1953277587890625, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [572/1000], Step [1/1], Loss: 0.202137753367424, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [573/1000], Step [1/1], Loss: 0.20824970304965973, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [574/1000], Step [1/1], Loss: 0.21429629623889923, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [575/1000], Step [1/1], Loss: 0.2188940793275833, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [576/1000], Step [1/1], Loss: 0.2223283052444458, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [577/1000], Step [1/1], Loss: 0.22543053328990936, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [578/1000], Step [1/1], Loss: 0.2239912450313568, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [579/1000], Step [1/1], Loss: 0.2220146209001541, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [580/1000], Step [1/1], Loss: 0.21904516220092773, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [581/1000], Step [1/1], Loss: 0.20244638621807098, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [582/1000], Step [1/1], Loss: 0.18940283358097076, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [583/1000], Step [1/1], Loss: 0.18087351322174072, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [584/1000], Step [1/1], Loss: 0.17455726861953735, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [585/1000], Step [1/1], Loss: 0.1693383753299713, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [586/1000], Step [1/1], Loss: 0.16541171073913574, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [587/1000], Step [1/1], Loss: 0.16276030242443085, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [588/1000], Step [1/1], Loss: 0.16097694635391235, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [589/1000], Step [1/1], Loss: 0.15968230366706848, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [590/1000], Step [1/1], Loss: 0.16012631356716156, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [591/1000], Step [1/1], Loss: 0.17066523432731628, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [592/1000], Step [1/1], Loss: 0.18680499494075775, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [593/1000], Step [1/1], Loss: 0.20529460906982422, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [594/1000], Step [1/1], Loss: 0.22490614652633667, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [595/1000], Step [1/1], Loss: 0.24509133398532867, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [596/1000], Step [1/1], Loss: 0.26423656940460205, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [597/1000], Step [1/1], Loss: 0.2816966474056244, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [598/1000], Step [1/1], Loss: 0.2989250123500824, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [599/1000], Step [1/1], Loss: 0.3155175745487213, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [600/1000], Step [1/1], Loss: 0.3312835395336151, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [601/1000], Step [1/1], Loss: 0.3434159755706787, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [602/1000], Step [1/1], Loss: 0.348415732383728, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [603/1000], Step [1/1], Loss: 0.33286988735198975, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [604/1000], Step [1/1], Loss: 0.2602638304233551, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [605/1000], Step [1/1], Loss: 0.205457866191864, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [606/1000], Step [1/1], Loss: 0.1691826581954956, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [607/1000], Step [1/1], Loss: 0.1492985486984253, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [608/1000], Step [1/1], Loss: 0.14095573127269745, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [609/1000], Step [1/1], Loss: 0.14021600782871246, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [610/1000], Step [1/1], Loss: 0.14410877227783203, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [611/1000], Step [1/1], Loss: 0.14762340486049652, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [612/1000], Step [1/1], Loss: 0.15140634775161743, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [613/1000], Step [1/1], Loss: 0.15504935383796692, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [614/1000], Step [1/1], Loss: 0.15691310167312622, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [615/1000], Step [1/1], Loss: 0.15606684982776642, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [616/1000], Step [1/1], Loss: 0.15504540503025055, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [617/1000], Step [1/1], Loss: 0.1499328911304474, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [618/1000], Step [1/1], Loss: 0.14487388730049133, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [619/1000], Step [1/1], Loss: 0.16078490018844604, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [620/1000], Step [1/1], Loss: 0.19423942267894745, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [621/1000], Step [1/1], Loss: 0.24172089993953705, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [622/1000], Step [1/1], Loss: 0.2988150417804718, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [623/1000], Step [1/1], Loss: 0.35594987869262695, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [624/1000], Step [1/1], Loss: 0.3907085955142975, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [625/1000], Step [1/1], Loss: 0.4234786331653595, Test Accuracy: 76.3157894736842%\n",
      "{i+1}Epoch [626/1000], Step [1/1], Loss: 0.4382355809211731, Test Accuracy: 76.3157894736842%\n",
      "{i+1}Epoch [627/1000], Step [1/1], Loss: 0.42322754859924316, Test Accuracy: 76.3157894736842%\n",
      "{i+1}Epoch [628/1000], Step [1/1], Loss: 0.40660664439201355, Test Accuracy: 76.3157894736842%\n",
      "{i+1}Epoch [629/1000], Step [1/1], Loss: 0.39203694462776184, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [630/1000], Step [1/1], Loss: 0.3792564868927002, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [631/1000], Step [1/1], Loss: 0.36728471517562866, Test Accuracy: 78.94736842105263%\n",
      "{i+1}Epoch [632/1000], Step [1/1], Loss: 0.3514881730079651, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [633/1000], Step [1/1], Loss: 0.3281017541885376, Test Accuracy: 81.57894736842105%\n",
      "{i+1}Epoch [634/1000], Step [1/1], Loss: 0.3045607805252075, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [635/1000], Step [1/1], Loss: 0.28484687209129333, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [636/1000], Step [1/1], Loss: 0.2595369219779968, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [637/1000], Step [1/1], Loss: 0.23894837498664856, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [638/1000], Step [1/1], Loss: 0.22221799194812775, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [639/1000], Step [1/1], Loss: 0.2065819501876831, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [640/1000], Step [1/1], Loss: 0.19400113821029663, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [641/1000], Step [1/1], Loss: 0.18371634185314178, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [642/1000], Step [1/1], Loss: 0.17537260055541992, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [643/1000], Step [1/1], Loss: 0.16848260164260864, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [644/1000], Step [1/1], Loss: 0.16276705265045166, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [645/1000], Step [1/1], Loss: 0.15845516324043274, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [646/1000], Step [1/1], Loss: 0.15507875382900238, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [647/1000], Step [1/1], Loss: 0.15228822827339172, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [648/1000], Step [1/1], Loss: 0.15001897513866425, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [649/1000], Step [1/1], Loss: 0.1490393728017807, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [650/1000], Step [1/1], Loss: 0.1483483761548996, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [651/1000], Step [1/1], Loss: 0.14784736931324005, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [652/1000], Step [1/1], Loss: 0.14745962619781494, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [653/1000], Step [1/1], Loss: 0.1524503231048584, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [654/1000], Step [1/1], Loss: 0.1595621258020401, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [655/1000], Step [1/1], Loss: 0.17075712978839874, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [656/1000], Step [1/1], Loss: 0.18340353667736053, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [657/1000], Step [1/1], Loss: 0.19681227207183838, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [658/1000], Step [1/1], Loss: 0.21018044650554657, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [659/1000], Step [1/1], Loss: 0.22371874749660492, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [660/1000], Step [1/1], Loss: 0.23651577532291412, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [661/1000], Step [1/1], Loss: 0.24888621270656586, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [662/1000], Step [1/1], Loss: 0.256279855966568, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [663/1000], Step [1/1], Loss: 0.26320862770080566, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [664/1000], Step [1/1], Loss: 0.26333507895469666, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [665/1000], Step [1/1], Loss: 0.2607891261577606, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [666/1000], Step [1/1], Loss: 0.2570207715034485, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [667/1000], Step [1/1], Loss: 0.24906866252422333, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [668/1000], Step [1/1], Loss: 0.2414359599351883, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [669/1000], Step [1/1], Loss: 0.23342260718345642, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [670/1000], Step [1/1], Loss: 0.22506265342235565, Test Accuracy: 84.21052631578948%\n",
      "{i+1}Epoch [671/1000], Step [1/1], Loss: 0.21752259135246277, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [672/1000], Step [1/1], Loss: 0.2101813703775406, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [673/1000], Step [1/1], Loss: 0.20372743904590607, Test Accuracy: 86.84210526315789%\n",
      "{i+1}Epoch [674/1000], Step [1/1], Loss: 0.19808617234230042, Test Accuracy: 89.47368421052632%\n",
      "{i+1}Epoch [675/1000], Step [1/1], Loss: 0.19313013553619385, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [676/1000], Step [1/1], Loss: 0.1883023977279663, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [677/1000], Step [1/1], Loss: 0.1839846521615982, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [678/1000], Step [1/1], Loss: 0.17934805154800415, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [679/1000], Step [1/1], Loss: 0.17534081637859344, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [680/1000], Step [1/1], Loss: 0.17193138599395752, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [681/1000], Step [1/1], Loss: 0.16894875466823578, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [682/1000], Step [1/1], Loss: 0.1662771999835968, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [683/1000], Step [1/1], Loss: 0.16393515467643738, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [684/1000], Step [1/1], Loss: 0.16220375895500183, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [685/1000], Step [1/1], Loss: 0.16083936393260956, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [686/1000], Step [1/1], Loss: 0.15979304909706116, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [687/1000], Step [1/1], Loss: 0.15894173085689545, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [688/1000], Step [1/1], Loss: 0.15809699892997742, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [689/1000], Step [1/1], Loss: 0.1587396115064621, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [690/1000], Step [1/1], Loss: 0.15966689586639404, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [691/1000], Step [1/1], Loss: 0.1610119640827179, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [692/1000], Step [1/1], Loss: 0.16215677559375763, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [693/1000], Step [1/1], Loss: 0.16504934430122375, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [694/1000], Step [1/1], Loss: 0.1685919314622879, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [695/1000], Step [1/1], Loss: 0.17223703861236572, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [696/1000], Step [1/1], Loss: 0.17589381337165833, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [697/1000], Step [1/1], Loss: 0.17933745682239532, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [698/1000], Step [1/1], Loss: 0.18250803649425507, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [699/1000], Step [1/1], Loss: 0.18527479469776154, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [700/1000], Step [1/1], Loss: 0.18782253563404083, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [701/1000], Step [1/1], Loss: 0.18957021832466125, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [702/1000], Step [1/1], Loss: 0.18940015137195587, Test Accuracy: 92.10526315789474%\n",
      "{i+1}Epoch [703/1000], Step [1/1], Loss: 0.18828223645687103, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [704/1000], Step [1/1], Loss: 0.18672297894954681, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [705/1000], Step [1/1], Loss: 0.18506228923797607, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [706/1000], Step [1/1], Loss: 0.18347544968128204, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [707/1000], Step [1/1], Loss: 0.18203963339328766, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [708/1000], Step [1/1], Loss: 0.1806643158197403, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [709/1000], Step [1/1], Loss: 0.17940060794353485, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [710/1000], Step [1/1], Loss: 0.1771826446056366, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [711/1000], Step [1/1], Loss: 0.17437057197093964, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [712/1000], Step [1/1], Loss: 0.17188656330108643, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [713/1000], Step [1/1], Loss: 0.16965638101100922, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [714/1000], Step [1/1], Loss: 0.16782326996326447, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [715/1000], Step [1/1], Loss: 0.16620853543281555, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [716/1000], Step [1/1], Loss: 0.16488493978977203, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [717/1000], Step [1/1], Loss: 0.16490787267684937, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [718/1000], Step [1/1], Loss: 0.16500318050384521, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [719/1000], Step [1/1], Loss: 0.16509561240673065, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [720/1000], Step [1/1], Loss: 0.16523461043834686, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [721/1000], Step [1/1], Loss: 0.1657707393169403, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [722/1000], Step [1/1], Loss: 0.16624139249324799, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [723/1000], Step [1/1], Loss: 0.16650176048278809, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [724/1000], Step [1/1], Loss: 0.1669730693101883, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [725/1000], Step [1/1], Loss: 0.1673411875963211, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [726/1000], Step [1/1], Loss: 0.1676904708147049, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [727/1000], Step [1/1], Loss: 0.16881079971790314, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [728/1000], Step [1/1], Loss: 0.1697709560394287, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [729/1000], Step [1/1], Loss: 0.17071504890918732, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [730/1000], Step [1/1], Loss: 0.1716066300868988, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [731/1000], Step [1/1], Loss: 0.17214547097682953, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [732/1000], Step [1/1], Loss: 0.17267991602420807, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [733/1000], Step [1/1], Loss: 0.17236003279685974, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [734/1000], Step [1/1], Loss: 0.17170071601867676, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [735/1000], Step [1/1], Loss: 0.17061395943164825, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [736/1000], Step [1/1], Loss: 0.169387549161911, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [737/1000], Step [1/1], Loss: 0.1676200032234192, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [738/1000], Step [1/1], Loss: 0.16646471619606018, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [739/1000], Step [1/1], Loss: 0.16537168622016907, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [740/1000], Step [1/1], Loss: 0.16439974308013916, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [741/1000], Step [1/1], Loss: 0.16358554363250732, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [742/1000], Step [1/1], Loss: 0.1628274917602539, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [743/1000], Step [1/1], Loss: 0.16223490238189697, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [744/1000], Step [1/1], Loss: 0.1626506745815277, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [745/1000], Step [1/1], Loss: 0.1631239950656891, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [746/1000], Step [1/1], Loss: 0.16353976726531982, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [747/1000], Step [1/1], Loss: 0.16398100554943085, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [748/1000], Step [1/1], Loss: 0.1649141013622284, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [749/1000], Step [1/1], Loss: 0.16567058861255646, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [750/1000], Step [1/1], Loss: 0.1663745641708374, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [751/1000], Step [1/1], Loss: 0.16688662767410278, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [752/1000], Step [1/1], Loss: 0.1675918698310852, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [753/1000], Step [1/1], Loss: 0.16824357211589813, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [754/1000], Step [1/1], Loss: 0.16885405778884888, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [755/1000], Step [1/1], Loss: 0.169439435005188, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [756/1000], Step [1/1], Loss: 0.16997462511062622, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [757/1000], Step [1/1], Loss: 0.17052291333675385, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [758/1000], Step [1/1], Loss: 0.17083488404750824, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [759/1000], Step [1/1], Loss: 0.17125682532787323, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [760/1000], Step [1/1], Loss: 0.17090512812137604, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [761/1000], Step [1/1], Loss: 0.17061243951320648, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [762/1000], Step [1/1], Loss: 0.17049169540405273, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [763/1000], Step [1/1], Loss: 0.1697230339050293, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [764/1000], Step [1/1], Loss: 0.1691567748785019, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [765/1000], Step [1/1], Loss: 0.16863924264907837, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [766/1000], Step [1/1], Loss: 0.16818511486053467, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [767/1000], Step [1/1], Loss: 0.16737930476665497, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [768/1000], Step [1/1], Loss: 0.16618230938911438, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [769/1000], Step [1/1], Loss: 0.1647782325744629, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [770/1000], Step [1/1], Loss: 0.16351859271526337, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [771/1000], Step [1/1], Loss: 0.16240888833999634, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [772/1000], Step [1/1], Loss: 0.161295086145401, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [773/1000], Step [1/1], Loss: 0.16032660007476807, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [774/1000], Step [1/1], Loss: 0.15945784747600555, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [775/1000], Step [1/1], Loss: 0.1585647165775299, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [776/1000], Step [1/1], Loss: 0.15773165225982666, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [777/1000], Step [1/1], Loss: 0.15744830667972565, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [778/1000], Step [1/1], Loss: 0.15735308825969696, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [779/1000], Step [1/1], Loss: 0.1575120985507965, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [780/1000], Step [1/1], Loss: 0.15773005783557892, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [781/1000], Step [1/1], Loss: 0.15822991728782654, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [782/1000], Step [1/1], Loss: 0.1590014100074768, Test Accuracy: 100.0%\n",
      "{i+1}Epoch [783/1000], Step [1/1], Loss: 0.15970058739185333, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [784/1000], Step [1/1], Loss: 0.16041307151317596, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [785/1000], Step [1/1], Loss: 0.16106373071670532, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [786/1000], Step [1/1], Loss: 0.16161108016967773, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [787/1000], Step [1/1], Loss: 0.16209986805915833, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [788/1000], Step [1/1], Loss: 0.16247856616973877, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [789/1000], Step [1/1], Loss: 0.16281147301197052, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [790/1000], Step [1/1], Loss: 0.16335429251194, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [791/1000], Step [1/1], Loss: 0.16378843784332275, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [792/1000], Step [1/1], Loss: 0.16423507034778595, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [793/1000], Step [1/1], Loss: 0.16472792625427246, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [794/1000], Step [1/1], Loss: 0.16523055732250214, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [795/1000], Step [1/1], Loss: 0.16580452024936676, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [796/1000], Step [1/1], Loss: 0.16645076870918274, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [797/1000], Step [1/1], Loss: 0.1668122410774231, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [798/1000], Step [1/1], Loss: 0.1667763888835907, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [799/1000], Step [1/1], Loss: 0.16677211225032806, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [800/1000], Step [1/1], Loss: 0.16681714355945587, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [801/1000], Step [1/1], Loss: 0.16689494252204895, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [802/1000], Step [1/1], Loss: 0.1674160212278366, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [803/1000], Step [1/1], Loss: 0.16804243624210358, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [804/1000], Step [1/1], Loss: 0.16862979531288147, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [805/1000], Step [1/1], Loss: 0.1686377078294754, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [806/1000], Step [1/1], Loss: 0.16859395802021027, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [807/1000], Step [1/1], Loss: 0.16857993602752686, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [808/1000], Step [1/1], Loss: 0.16856226325035095, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [809/1000], Step [1/1], Loss: 0.16843314468860626, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [810/1000], Step [1/1], Loss: 0.1677646040916443, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [811/1000], Step [1/1], Loss: 0.16710920631885529, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [812/1000], Step [1/1], Loss: 0.16663815081119537, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [813/1000], Step [1/1], Loss: 0.1658576875925064, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [814/1000], Step [1/1], Loss: 0.16511012613773346, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [815/1000], Step [1/1], Loss: 0.164354145526886, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [816/1000], Step [1/1], Loss: 0.16358445584774017, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [817/1000], Step [1/1], Loss: 0.16278256475925446, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [818/1000], Step [1/1], Loss: 0.16190701723098755, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [819/1000], Step [1/1], Loss: 0.16126446425914764, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [820/1000], Step [1/1], Loss: 0.16076155006885529, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [821/1000], Step [1/1], Loss: 0.16030699014663696, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [822/1000], Step [1/1], Loss: 0.15988989174365997, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [823/1000], Step [1/1], Loss: 0.16023260354995728, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [824/1000], Step [1/1], Loss: 0.1605270802974701, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [825/1000], Step [1/1], Loss: 0.16084657609462738, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [826/1000], Step [1/1], Loss: 0.16134808957576752, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [827/1000], Step [1/1], Loss: 0.16178381443023682, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [828/1000], Step [1/1], Loss: 0.1623295247554779, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [829/1000], Step [1/1], Loss: 0.16282252967357635, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [830/1000], Step [1/1], Loss: 0.16272270679473877, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [831/1000], Step [1/1], Loss: 0.16308915615081787, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [832/1000], Step [1/1], Loss: 0.16391806304454803, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [833/1000], Step [1/1], Loss: 0.16417525708675385, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [834/1000], Step [1/1], Loss: 0.16429315507411957, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [835/1000], Step [1/1], Loss: 0.16432934999465942, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [836/1000], Step [1/1], Loss: 0.16478413343429565, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [837/1000], Step [1/1], Loss: 0.16397865116596222, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [838/1000], Step [1/1], Loss: 0.16327810287475586, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [839/1000], Step [1/1], Loss: 0.16174764931201935, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [840/1000], Step [1/1], Loss: 0.16015850007534027, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [841/1000], Step [1/1], Loss: 0.15871839225292206, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [842/1000], Step [1/1], Loss: 0.157880961894989, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [843/1000], Step [1/1], Loss: 0.15712137520313263, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [844/1000], Step [1/1], Loss: 0.1569177657365799, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [845/1000], Step [1/1], Loss: 0.1571408361196518, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [846/1000], Step [1/1], Loss: 0.157173290848732, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [847/1000], Step [1/1], Loss: 0.1573341339826584, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [848/1000], Step [1/1], Loss: 0.15753017365932465, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [849/1000], Step [1/1], Loss: 0.1578885167837143, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [850/1000], Step [1/1], Loss: 0.15820921957492828, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [851/1000], Step [1/1], Loss: 0.15845371782779694, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [852/1000], Step [1/1], Loss: 0.15858414769172668, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [853/1000], Step [1/1], Loss: 0.15901066362857819, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [854/1000], Step [1/1], Loss: 0.1599002629518509, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [855/1000], Step [1/1], Loss: 0.16090932488441467, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [856/1000], Step [1/1], Loss: 0.16152647137641907, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [857/1000], Step [1/1], Loss: 0.16148479282855988, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [858/1000], Step [1/1], Loss: 0.1614537537097931, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [859/1000], Step [1/1], Loss: 0.16124440729618073, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [860/1000], Step [1/1], Loss: 0.16148795187473297, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [861/1000], Step [1/1], Loss: 0.16167524456977844, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [862/1000], Step [1/1], Loss: 0.16193005442619324, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [863/1000], Step [1/1], Loss: 0.16108866035938263, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [864/1000], Step [1/1], Loss: 0.1608450710773468, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [865/1000], Step [1/1], Loss: 0.1606864482164383, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [866/1000], Step [1/1], Loss: 0.16051600873470306, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [867/1000], Step [1/1], Loss: 0.15924595296382904, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [868/1000], Step [1/1], Loss: 0.15805405378341675, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [869/1000], Step [1/1], Loss: 0.15699611604213715, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [870/1000], Step [1/1], Loss: 0.15615729987621307, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [871/1000], Step [1/1], Loss: 0.1558026522397995, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [872/1000], Step [1/1], Loss: 0.15585660934448242, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [873/1000], Step [1/1], Loss: 0.15626876056194305, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [874/1000], Step [1/1], Loss: 0.1564282923936844, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [875/1000], Step [1/1], Loss: 0.15660670399665833, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [876/1000], Step [1/1], Loss: 0.1572432667016983, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [877/1000], Step [1/1], Loss: 0.15755896270275116, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [878/1000], Step [1/1], Loss: 0.15832428634166718, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [879/1000], Step [1/1], Loss: 0.158839613199234, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [880/1000], Step [1/1], Loss: 0.15938451886177063, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [881/1000], Step [1/1], Loss: 0.15890610218048096, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [882/1000], Step [1/1], Loss: 0.15853171050548553, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [883/1000], Step [1/1], Loss: 0.15817204117774963, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [884/1000], Step [1/1], Loss: 0.1577574908733368, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [885/1000], Step [1/1], Loss: 0.15751492977142334, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [886/1000], Step [1/1], Loss: 0.15736716985702515, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [887/1000], Step [1/1], Loss: 0.15776635706424713, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [888/1000], Step [1/1], Loss: 0.15790553390979767, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [889/1000], Step [1/1], Loss: 0.15780293941497803, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [890/1000], Step [1/1], Loss: 0.157475084066391, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [891/1000], Step [1/1], Loss: 0.1564941257238388, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [892/1000], Step [1/1], Loss: 0.15556691586971283, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [893/1000], Step [1/1], Loss: 0.15477915108203888, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [894/1000], Step [1/1], Loss: 0.1540943831205368, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [895/1000], Step [1/1], Loss: 0.153830423951149, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [896/1000], Step [1/1], Loss: 0.15341292321681976, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [897/1000], Step [1/1], Loss: 0.15299883484840393, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [898/1000], Step [1/1], Loss: 0.15261653065681458, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [899/1000], Step [1/1], Loss: 0.15255874395370483, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [900/1000], Step [1/1], Loss: 0.15154945850372314, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [901/1000], Step [1/1], Loss: 0.15091028809547424, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [902/1000], Step [1/1], Loss: 0.15031284093856812, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [903/1000], Step [1/1], Loss: 0.14975561201572418, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [904/1000], Step [1/1], Loss: 0.14926819503307343, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [905/1000], Step [1/1], Loss: 0.14891493320465088, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [906/1000], Step [1/1], Loss: 0.15065698325634003, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [907/1000], Step [1/1], Loss: 0.15232820808887482, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [908/1000], Step [1/1], Loss: 0.1538476198911667, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [909/1000], Step [1/1], Loss: 0.1558140367269516, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [910/1000], Step [1/1], Loss: 0.1571081131696701, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [911/1000], Step [1/1], Loss: 0.15828672051429749, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [912/1000], Step [1/1], Loss: 0.15926393866539001, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [913/1000], Step [1/1], Loss: 0.1594911515712738, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [914/1000], Step [1/1], Loss: 0.15958553552627563, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [915/1000], Step [1/1], Loss: 0.15977679193019867, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [916/1000], Step [1/1], Loss: 0.15935654938220978, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [917/1000], Step [1/1], Loss: 0.15865711867809296, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [918/1000], Step [1/1], Loss: 0.15746289491653442, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [919/1000], Step [1/1], Loss: 0.15625140070915222, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [920/1000], Step [1/1], Loss: 0.15516942739486694, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [921/1000], Step [1/1], Loss: 0.15438000857830048, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [922/1000], Step [1/1], Loss: 0.15367600321769714, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [923/1000], Step [1/1], Loss: 0.15359759330749512, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [924/1000], Step [1/1], Loss: 0.15428617596626282, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [925/1000], Step [1/1], Loss: 0.15454891324043274, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [926/1000], Step [1/1], Loss: 0.15483607351779938, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [927/1000], Step [1/1], Loss: 0.15522785484790802, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [928/1000], Step [1/1], Loss: 0.15518157184123993, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [929/1000], Step [1/1], Loss: 0.15496774017810822, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [930/1000], Step [1/1], Loss: 0.15474286675453186, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [931/1000], Step [1/1], Loss: 0.15455392003059387, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [932/1000], Step [1/1], Loss: 0.1546415537595749, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [933/1000], Step [1/1], Loss: 0.1545698195695877, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [934/1000], Step [1/1], Loss: 0.153954416513443, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [935/1000], Step [1/1], Loss: 0.15365980565547943, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [936/1000], Step [1/1], Loss: 0.15252785384655, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [937/1000], Step [1/1], Loss: 0.15151068568229675, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [938/1000], Step [1/1], Loss: 0.15042980015277863, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [939/1000], Step [1/1], Loss: 0.14938585460186005, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [940/1000], Step [1/1], Loss: 0.14895325899124146, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [941/1000], Step [1/1], Loss: 0.14862217009067535, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [942/1000], Step [1/1], Loss: 0.1483629196882248, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [943/1000], Step [1/1], Loss: 0.14918865263462067, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [944/1000], Step [1/1], Loss: 0.1509159505367279, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [945/1000], Step [1/1], Loss: 0.15232235193252563, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [946/1000], Step [1/1], Loss: 0.15351727604866028, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [947/1000], Step [1/1], Loss: 0.15517722070217133, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [948/1000], Step [1/1], Loss: 0.15681418776512146, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [949/1000], Step [1/1], Loss: 0.158060684800148, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [950/1000], Step [1/1], Loss: 0.15862955152988434, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [951/1000], Step [1/1], Loss: 0.1589766889810562, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [952/1000], Step [1/1], Loss: 0.15903569757938385, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [953/1000], Step [1/1], Loss: 0.15789082646369934, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [954/1000], Step [1/1], Loss: 0.15538586676120758, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [955/1000], Step [1/1], Loss: 0.15272092819213867, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [956/1000], Step [1/1], Loss: 0.1502862572669983, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [957/1000], Step [1/1], Loss: 0.14810699224472046, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [958/1000], Step [1/1], Loss: 0.15000949800014496, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [959/1000], Step [1/1], Loss: 0.15139153599739075, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [960/1000], Step [1/1], Loss: 0.15075482428073883, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [961/1000], Step [1/1], Loss: 0.15031373500823975, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [962/1000], Step [1/1], Loss: 0.15078520774841309, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [963/1000], Step [1/1], Loss: 0.15204685926437378, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [964/1000], Step [1/1], Loss: 0.1523178219795227, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [965/1000], Step [1/1], Loss: 0.15227264165878296, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [966/1000], Step [1/1], Loss: 0.1515449732542038, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [967/1000], Step [1/1], Loss: 0.15088416635990143, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [968/1000], Step [1/1], Loss: 0.15029466152191162, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [969/1000], Step [1/1], Loss: 0.1491888016462326, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [970/1000], Step [1/1], Loss: 0.14833880960941315, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [971/1000], Step [1/1], Loss: 0.14775031805038452, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [972/1000], Step [1/1], Loss: 0.14725269377231598, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [973/1000], Step [1/1], Loss: 0.14694525301456451, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [974/1000], Step [1/1], Loss: 0.14694927632808685, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [975/1000], Step [1/1], Loss: 0.14672037959098816, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [976/1000], Step [1/1], Loss: 0.14721126854419708, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [977/1000], Step [1/1], Loss: 0.14727531373500824, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [978/1000], Step [1/1], Loss: 0.14737768471240997, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [979/1000], Step [1/1], Loss: 0.14762647449970245, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [980/1000], Step [1/1], Loss: 0.14772135019302368, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [981/1000], Step [1/1], Loss: 0.1476110816001892, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [982/1000], Step [1/1], Loss: 0.1470685750246048, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [983/1000], Step [1/1], Loss: 0.14660343527793884, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [984/1000], Step [1/1], Loss: 0.14713430404663086, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [985/1000], Step [1/1], Loss: 0.147164449095726, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [986/1000], Step [1/1], Loss: 0.14729642868041992, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [987/1000], Step [1/1], Loss: 0.1474263221025467, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [988/1000], Step [1/1], Loss: 0.1475362330675125, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [989/1000], Step [1/1], Loss: 0.14814400672912598, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [990/1000], Step [1/1], Loss: 0.14868946373462677, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [991/1000], Step [1/1], Loss: 0.1492019146680832, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [992/1000], Step [1/1], Loss: 0.14914266765117645, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [993/1000], Step [1/1], Loss: 0.1477086842060089, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [994/1000], Step [1/1], Loss: 0.14625440537929535, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [995/1000], Step [1/1], Loss: 0.14485888183116913, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [996/1000], Step [1/1], Loss: 0.1431082934141159, Test Accuracy: 94.73684210526316%\n",
      "{i+1}Epoch [997/1000], Step [1/1], Loss: 0.14154447615146637, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [998/1000], Step [1/1], Loss: 0.1406330019235611, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [999/1000], Step [1/1], Loss: 0.13993260264396667, Test Accuracy: 97.36842105263158%\n",
      "{i+1}Epoch [1000/1000], Step [1/1], Loss: 0.1397569328546524, Test Accuracy: 97.36842105263158%\n"
     ]
    }
   ],
   "source": [
    "#use SPSA to optimize The Neural network\n",
    "n_epochs =1000\n",
    "NN_IRIS = Neural_Net(input_size=4, hidden_size=10, n_classes=3)\n",
    "N_dim = NN_IRIS.count_parameters()\n",
    "#specify we don't need the computation graph to keep track of the gradients, we will use SPSA to update the weights\n",
    "with torch.no_grad():\n",
    "    for param in NN_IRIS.parameters():\n",
    "        param.requires_grad = False\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# learning parameters\n",
    "\n",
    "init_pos = NN_IRIS.get_params()\n",
    "\n",
    "if init_pos.requires_grad:\n",
    "    # Detach the tensor from the computation graph\n",
    "    init_pos = init_pos.detach()\n",
    "if init_pos.is_cuda:\n",
    "    # Move the tensor to the CPU\n",
    "    init_pos = init_pos.cpu()\n",
    "init_pos = init_pos.numpy()\n",
    "\n",
    "SPSA_optimizer = SPSA_opt(init_pos,alpha=1e-3,epsilon=1e-5)\n",
    "Adam = AdamOptimizer(init_pos, lr=1e-2, beta1=0.9, beta2=0.99, epsilon=1e-8)\n",
    "\n",
    "test_acc_SPSA, best_reward_SPSA = train_online_SPSA_NN(NN_IRIS, n_epochs, Iris_train_loader, Iris_test_loader, loss, SPSA_optimizer,Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "PEPG",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999
         ],
         "y": [
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          34.21052631578947,
          31.57894736842105,
          26.31578947368421,
          28.94736842105263,
          21.05263157894737,
          21.05263157894737,
          26.31578947368421,
          34.21052631578947,
          42.10526315789474,
          42.10526315789474,
          44.73684210526316,
          34.21052631578947,
          28.94736842105263,
          28.94736842105263,
          26.31578947368421,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          23.68421052631579,
          26.31578947368421,
          26.31578947368421,
          42.10526315789474,
          42.10526315789474,
          50,
          52.63157894736842,
          50,
          50,
          55.26315789473684,
          68.42105263157895,
          71.05263157894737,
          78.94736842105263,
          81.57894736842105,
          84.21052631578948,
          81.57894736842105,
          84.21052631578948,
          81.57894736842105,
          86.84210526315789,
          89.47368421052632,
          89.47368421052632,
          89.47368421052632,
          89.47368421052632,
          89.47368421052632,
          89.47368421052632,
          89.47368421052632,
          86.84210526315789,
          86.84210526315789,
          92.10526315789474,
          94.73684210526316,
          92.10526315789474,
          92.10526315789474,
          89.47368421052632,
          86.84210526315789,
          86.84210526315789,
          86.84210526315789,
          86.84210526315789,
          84.21052631578948,
          81.57894736842105,
          81.57894736842105,
          81.57894736842105,
          84.21052631578948,
          86.84210526315789,
          86.84210526315789,
          89.47368421052632,
          89.47368421052632,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          89.47368421052632,
          89.47368421052632,
          89.47368421052632,
          89.47368421052632,
          86.84210526315789,
          86.84210526315789,
          81.57894736842105,
          86.84210526315789,
          89.47368421052632,
          92.10526315789474,
          92.10526315789474,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          92.10526315789474,
          92.10526315789474,
          89.47368421052632,
          89.47368421052632,
          89.47368421052632,
          86.84210526315789,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          86.84210526315789,
          89.47368421052632,
          89.47368421052632,
          89.47368421052632,
          92.10526315789474,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          100,
          100,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          100,
          100,
          100,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          92.10526315789474,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          100,
          100,
          100,
          100,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          100,
          100,
          100,
          100,
          100,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          100,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          92.10526315789474,
          89.47368421052632,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          100,
          100,
          100,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          89.47368421052632,
          89.47368421052632,
          89.47368421052632,
          89.47368421052632,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          94.73684210526316,
          94.73684210526316,
          86.84210526315789,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          81.57894736842105,
          81.57894736842105,
          78.94736842105263,
          78.94736842105263,
          78.94736842105263,
          84.21052631578948,
          94.73684210526316,
          100,
          100,
          100,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          100,
          100,
          97.36842105263158,
          84.21052631578948,
          84.21052631578948,
          81.57894736842105,
          78.94736842105263,
          76.3157894736842,
          76.3157894736842,
          76.3157894736842,
          76.3157894736842,
          78.94736842105263,
          78.94736842105263,
          78.94736842105263,
          81.57894736842105,
          81.57894736842105,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          86.84210526315789,
          89.47368421052632,
          94.73684210526316,
          97.36842105263158,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          97.36842105263158,
          94.73684210526316,
          86.84210526315789,
          86.84210526315789,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          84.21052631578948,
          86.84210526315789,
          86.84210526315789,
          86.84210526315789,
          89.47368421052632,
          92.10526315789474,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          92.10526315789474,
          92.10526315789474,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          100,
          100,
          100,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          100,
          100,
          100,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          100,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          94.73684210526316,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158,
          97.36842105263158
         ]
        }
       ],
       "layout": {
        "height": 300,
        "margin": {
         "b": 20,
         "l": 20,
         "r": 20,
         "t": 20
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "width": 400,
        "xaxis": {
         "title": {
          "text": "Epochs"
         },
         "type": "log"
        },
        "yaxis": {
         "title": {
          "text": "Accuracy [%]"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(test_acc_SPSA)), y=test_acc_SPSA, mode='lines', name='PEPG'))\n",
    "#change theme to white and set the sizer of the plot\n",
    "fig.update_layout(template='plotly_white', width=400, height=300,margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig.update_xaxes(title_text=\"Epochs\",type = 'log')\n",
    "fig.update_yaxes(title_text=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Neural_Net(\n",
      "  (NN_stack): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=4, out_features=10, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "{i+1}Epoch [1/500], Step [1/1], Loss: 1.1106672286987305, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [2/500], Step [1/1], Loss: 1.0980488061904907, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [3/500], Step [1/1], Loss: 1.0857322216033936, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [4/500], Step [1/1], Loss: 1.072855830192566, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [5/500], Step [1/1], Loss: 1.0610471963882446, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [6/500], Step [1/1], Loss: 1.0514963865280151, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [7/500], Step [1/1], Loss: 1.0470468997955322, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [8/500], Step [1/1], Loss: 1.0497366189956665, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [9/500], Step [1/1], Loss: 1.0561449527740479, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [10/500], Step [1/1], Loss: 1.064699649810791, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [11/500], Step [1/1], Loss: 1.0737724304199219, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [12/500], Step [1/1], Loss: 1.0781317949295044, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [13/500], Step [1/1], Loss: 1.0784395933151245, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [14/500], Step [1/1], Loss: 1.078984260559082, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [15/500], Step [1/1], Loss: 1.0809589624404907, Test Accuracy: 44.73684210526316%\n",
      "{i+1}Epoch [16/500], Step [1/1], Loss: 1.0844424962997437, Test Accuracy: 42.10526315789474%\n",
      "{i+1}Epoch [17/500], Step [1/1], Loss: 1.0879608392715454, Test Accuracy: 39.473684210526315%\n",
      "{i+1}Epoch [18/500], Step [1/1], Loss: 1.0869096517562866, Test Accuracy: 28.94736842105263%\n",
      "{i+1}Epoch [19/500], Step [1/1], Loss: 1.0903934240341187, Test Accuracy: 23.68421052631579%\n",
      "{i+1}Epoch [20/500], Step [1/1], Loss: 1.1018037796020508, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [21/500], Step [1/1], Loss: 1.1112674474716187, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [22/500], Step [1/1], Loss: 1.1187031269073486, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [23/500], Step [1/1], Loss: 1.1235061883926392, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [24/500], Step [1/1], Loss: 1.1251134872436523, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [25/500], Step [1/1], Loss: 1.1231492757797241, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [26/500], Step [1/1], Loss: 1.1205734014511108, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [27/500], Step [1/1], Loss: 1.117709755897522, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [28/500], Step [1/1], Loss: 1.1145375967025757, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [29/500], Step [1/1], Loss: 1.111043095588684, Test Accuracy: 21.05263157894737%\n",
      "{i+1}Epoch [30/500], Step [1/1], Loss: 1.10721755027771, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [31/500], Step [1/1], Loss: 1.103067398071289, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [32/500], Step [1/1], Loss: 1.0986167192459106, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [33/500], Step [1/1], Loss: 1.0939157009124756, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [34/500], Step [1/1], Loss: 1.0890558958053589, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [35/500], Step [1/1], Loss: 1.0841795206069946, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [36/500], Step [1/1], Loss: 1.079501748085022, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [37/500], Step [1/1], Loss: 1.0753350257873535, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [38/500], Step [1/1], Loss: 1.072113037109375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [39/500], Step [1/1], Loss: 1.070429801940918, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [40/500], Step [1/1], Loss: 1.071070671081543, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [41/500], Step [1/1], Loss: 1.075049638748169, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [42/500], Step [1/1], Loss: 1.08363938331604, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [43/500], Step [1/1], Loss: 1.0983837842941284, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [44/500], Step [1/1], Loss: 1.1210920810699463, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [45/500], Step [1/1], Loss: 1.1537994146347046, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [46/500], Step [1/1], Loss: 1.2212384939193726, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [47/500], Step [1/1], Loss: 1.3142319917678833, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [48/500], Step [1/1], Loss: 1.4339792728424072, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [49/500], Step [1/1], Loss: 1.581952691078186, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [50/500], Step [1/1], Loss: 1.7599784135818481, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [51/500], Step [1/1], Loss: 1.9707447290420532, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [52/500], Step [1/1], Loss: 2.2181313037872314, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [53/500], Step [1/1], Loss: 2.5073719024658203, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [54/500], Step [1/1], Loss: 2.845080852508545, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [55/500], Step [1/1], Loss: 3.2392868995666504, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [56/500], Step [1/1], Loss: 3.6995222568511963, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [57/500], Step [1/1], Loss: 4.236996173858643, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [58/500], Step [1/1], Loss: 4.864859104156494, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [59/500], Step [1/1], Loss: 5.598519325256348, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [60/500], Step [1/1], Loss: 6.456043243408203, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [61/500], Step [1/1], Loss: 7.458615779876709, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [62/500], Step [1/1], Loss: 8.631070137023926, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [63/500], Step [1/1], Loss: 10.002545356750488, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [64/500], Step [1/1], Loss: 11.607205390930176, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [65/500], Step [1/1], Loss: 13.485154151916504, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [66/500], Step [1/1], Loss: 15.683408737182617, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [67/500], Step [1/1], Loss: 18.25718116760254, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [68/500], Step [1/1], Loss: 21.271263122558594, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [69/500], Step [1/1], Loss: 24.801700592041016, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [70/500], Step [1/1], Loss: 28.937763214111328, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [71/500], Step [1/1], Loss: 33.78430938720703, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [72/500], Step [1/1], Loss: 39.464412689208984, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [73/500], Step [1/1], Loss: 46.12261199951172, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [74/500], Step [1/1], Loss: 53.9287109375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [75/500], Step [1/1], Loss: 63.08217239379883, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [76/500], Step [1/1], Loss: 73.81721496582031, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [77/500], Step [1/1], Loss: 86.40906524658203, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [78/500], Step [1/1], Loss: 101.18111419677734, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [79/500], Step [1/1], Loss: 118.51322174072266, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [80/500], Step [1/1], Loss: 138.85183715820312, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [81/500], Step [1/1], Loss: 162.72132873535156, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [82/500], Step [1/1], Loss: 190.73806762695312, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [83/500], Step [1/1], Loss: 223.62608337402344, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [84/500], Step [1/1], Loss: 262.23583984375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [85/500], Step [1/1], Loss: 307.5669250488281, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [86/500], Step [1/1], Loss: 360.79290771484375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [87/500], Step [1/1], Loss: 423.2923583984375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [88/500], Step [1/1], Loss: 496.68255615234375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [89/500], Step [1/1], Loss: 582.8624877929688, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [90/500], Step [1/1], Loss: 684.0603637695312, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [91/500], Step [1/1], Loss: 802.885009765625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [92/500], Step [1/1], Loss: 942.3949584960938, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [93/500], Step [1/1], Loss: 1106.1700439453125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [94/500], Step [1/1], Loss: 1298.393798828125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [95/500], Step [1/1], Loss: 1523.9552001953125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [96/500], Step [1/1], Loss: 1788.5540771484375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [97/500], Step [1/1], Loss: 2098.830078125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [98/500], Step [1/1], Loss: 2462.494140625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [99/500], Step [1/1], Loss: 2888.4951171875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [100/500], Step [1/1], Loss: 3387.165771484375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [101/500], Step [1/1], Loss: 3970.417236328125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [102/500], Step [1/1], Loss: 4651.9013671875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [103/500], Step [1/1], Loss: 5447.20166015625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [104/500], Step [1/1], Loss: 6373.99609375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [105/500], Step [1/1], Loss: 7452.19140625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [106/500], Step [1/1], Loss: 8703.998046875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [107/500], Step [1/1], Loss: 10153.9697265625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [108/500], Step [1/1], Loss: 11828.8896484375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [109/500], Step [1/1], Loss: 13757.5771484375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [110/500], Step [1/1], Loss: 15970.408203125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [111/500], Step [1/1], Loss: 18498.828125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [112/500], Step [1/1], Loss: 21374.388671875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [113/500], Step [1/1], Loss: 24627.7734375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [114/500], Step [1/1], Loss: 28287.4765625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [115/500], Step [1/1], Loss: 32378.443359375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [116/500], Step [1/1], Loss: 36920.47265625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [117/500], Step [1/1], Loss: 41926.98828125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [118/500], Step [1/1], Loss: 47403.72265625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [119/500], Step [1/1], Loss: 53347.98046875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [120/500], Step [1/1], Loss: 59748.17578125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [121/500], Step [1/1], Loss: 66584.1953125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [122/500], Step [1/1], Loss: 73827.9765625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [123/500], Step [1/1], Loss: 81444.65625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [124/500], Step [1/1], Loss: 89393.953125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [125/500], Step [1/1], Loss: 97632.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [126/500], Step [1/1], Loss: 106112.671875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [127/500], Step [1/1], Loss: 114789.0390625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [128/500], Step [1/1], Loss: 123614.8125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [129/500], Step [1/1], Loss: 132544.890625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [130/500], Step [1/1], Loss: 141536.234375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [131/500], Step [1/1], Loss: 150548.1875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [132/500], Step [1/1], Loss: 159542.125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [133/500], Step [1/1], Loss: 168482.140625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [134/500], Step [1/1], Loss: 177334.109375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [135/500], Step [1/1], Loss: 186065.953125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [136/500], Step [1/1], Loss: 194647.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [137/500], Step [1/1], Loss: 203047.921875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [138/500], Step [1/1], Loss: 211240.390625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [139/500], Step [1/1], Loss: 219197.296875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [140/500], Step [1/1], Loss: 226892.734375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [141/500], Step [1/1], Loss: 234301.890625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [142/500], Step [1/1], Loss: 241401.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [143/500], Step [1/1], Loss: 248172.03125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [144/500], Step [1/1], Loss: 254594.96875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [145/500], Step [1/1], Loss: 260656.453125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [146/500], Step [1/1], Loss: 266346.71875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [147/500], Step [1/1], Loss: 271661.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [148/500], Step [1/1], Loss: 276599.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [149/500], Step [1/1], Loss: 281168.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [150/500], Step [1/1], Loss: 285378.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [151/500], Step [1/1], Loss: 289243.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [152/500], Step [1/1], Loss: 292783.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [153/500], Step [1/1], Loss: 296021.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [154/500], Step [1/1], Loss: 298980.1875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [155/500], Step [1/1], Loss: 301685.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [156/500], Step [1/1], Loss: 304161.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [157/500], Step [1/1], Loss: 306433.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [158/500], Step [1/1], Loss: 308525.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [159/500], Step [1/1], Loss: 310457.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [160/500], Step [1/1], Loss: 312250.6875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [161/500], Step [1/1], Loss: 313922.3125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [162/500], Step [1/1], Loss: 315488.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [163/500], Step [1/1], Loss: 316963.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [164/500], Step [1/1], Loss: 318358.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [165/500], Step [1/1], Loss: 319686.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [166/500], Step [1/1], Loss: 320953.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [167/500], Step [1/1], Loss: 322168.5625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [168/500], Step [1/1], Loss: 323338.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [169/500], Step [1/1], Loss: 324469.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [170/500], Step [1/1], Loss: 325564.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [171/500], Step [1/1], Loss: 326629.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [172/500], Step [1/1], Loss: 327666.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [173/500], Step [1/1], Loss: 328678.21875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [174/500], Step [1/1], Loss: 329668.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [175/500], Step [1/1], Loss: 330637.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [176/500], Step [1/1], Loss: 331588.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [177/500], Step [1/1], Loss: 332522.03125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [178/500], Step [1/1], Loss: 333439.5625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [179/500], Step [1/1], Loss: 334342.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [180/500], Step [1/1], Loss: 335230.8125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [181/500], Step [1/1], Loss: 336106.1875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [182/500], Step [1/1], Loss: 336968.96875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [183/500], Step [1/1], Loss: 337819.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [184/500], Step [1/1], Loss: 338658.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [185/500], Step [1/1], Loss: 339486.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [186/500], Step [1/1], Loss: 340302.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [187/500], Step [1/1], Loss: 341109.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [188/500], Step [1/1], Loss: 341905.5, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [189/500], Step [1/1], Loss: 342691.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [190/500], Step [1/1], Loss: 343468.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [191/500], Step [1/1], Loss: 344234.96875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [192/500], Step [1/1], Loss: 344992.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [193/500], Step [1/1], Loss: 345741.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [194/500], Step [1/1], Loss: 346480.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [195/500], Step [1/1], Loss: 347211.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [196/500], Step [1/1], Loss: 347933.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [197/500], Step [1/1], Loss: 348647.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [198/500], Step [1/1], Loss: 349352.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [199/500], Step [1/1], Loss: 350049.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [200/500], Step [1/1], Loss: 350738.6875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [201/500], Step [1/1], Loss: 351419.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [202/500], Step [1/1], Loss: 352092.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [203/500], Step [1/1], Loss: 352757.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [204/500], Step [1/1], Loss: 353415.21875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [205/500], Step [1/1], Loss: 354065.21875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [206/500], Step [1/1], Loss: 354707.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [207/500], Step [1/1], Loss: 355342.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [208/500], Step [1/1], Loss: 355970.65625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [209/500], Step [1/1], Loss: 356591.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [210/500], Step [1/1], Loss: 357205.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [211/500], Step [1/1], Loss: 357811.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [212/500], Step [1/1], Loss: 358411.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [213/500], Step [1/1], Loss: 359004.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [214/500], Step [1/1], Loss: 359590.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [215/500], Step [1/1], Loss: 360170.28125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [216/500], Step [1/1], Loss: 360743.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [217/500], Step [1/1], Loss: 361310.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [218/500], Step [1/1], Loss: 361870.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [219/500], Step [1/1], Loss: 362424.1875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [220/500], Step [1/1], Loss: 362971.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [221/500], Step [1/1], Loss: 363513.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [222/500], Step [1/1], Loss: 364048.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [223/500], Step [1/1], Loss: 364578.5625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [224/500], Step [1/1], Loss: 365102.21875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [225/500], Step [1/1], Loss: 365619.8125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [226/500], Step [1/1], Loss: 366131.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [227/500], Step [1/1], Loss: 366638.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [228/500], Step [1/1], Loss: 367139.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [229/500], Step [1/1], Loss: 367633.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [230/500], Step [1/1], Loss: 368123.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [231/500], Step [1/1], Loss: 368607.8125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [232/500], Step [1/1], Loss: 369086.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [233/500], Step [1/1], Loss: 369559.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [234/500], Step [1/1], Loss: 370028.3125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [235/500], Step [1/1], Loss: 370491.3125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [236/500], Step [1/1], Loss: 370949.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [237/500], Step [1/1], Loss: 371402.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [238/500], Step [1/1], Loss: 371850.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [239/500], Step [1/1], Loss: 372293.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [240/500], Step [1/1], Loss: 372731.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [241/500], Step [1/1], Loss: 373164.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [242/500], Step [1/1], Loss: 373593.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [243/500], Step [1/1], Loss: 374017.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [244/500], Step [1/1], Loss: 374436.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [245/500], Step [1/1], Loss: 374851.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [246/500], Step [1/1], Loss: 375261.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [247/500], Step [1/1], Loss: 375667.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [248/500], Step [1/1], Loss: 376068.34375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [249/500], Step [1/1], Loss: 376465.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [250/500], Step [1/1], Loss: 376857.6875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [251/500], Step [1/1], Loss: 377245.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [252/500], Step [1/1], Loss: 377629.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [253/500], Step [1/1], Loss: 378009.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [254/500], Step [1/1], Loss: 378385.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [255/500], Step [1/1], Loss: 378757.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [256/500], Step [1/1], Loss: 379125.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [257/500], Step [1/1], Loss: 379488.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [258/500], Step [1/1], Loss: 379848.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [259/500], Step [1/1], Loss: 380204.4375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [260/500], Step [1/1], Loss: 380556.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [261/500], Step [1/1], Loss: 380904.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [262/500], Step [1/1], Loss: 381249.4375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [263/500], Step [1/1], Loss: 381590.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [264/500], Step [1/1], Loss: 381927.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [265/500], Step [1/1], Loss: 382260.875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [266/500], Step [1/1], Loss: 382590.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [267/500], Step [1/1], Loss: 382917.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [268/500], Step [1/1], Loss: 383239.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [269/500], Step [1/1], Loss: 383559.5, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [270/500], Step [1/1], Loss: 383875.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [271/500], Step [1/1], Loss: 384188.21875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [272/500], Step [1/1], Loss: 384497.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [273/500], Step [1/1], Loss: 384803.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [274/500], Step [1/1], Loss: 385106.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [275/500], Step [1/1], Loss: 385405.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [276/500], Step [1/1], Loss: 385701.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [277/500], Step [1/1], Loss: 385994.96875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [278/500], Step [1/1], Loss: 386284.96875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [279/500], Step [1/1], Loss: 386571.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [280/500], Step [1/1], Loss: 386855.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [281/500], Step [1/1], Loss: 387136.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [282/500], Step [1/1], Loss: 387414.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [283/500], Step [1/1], Loss: 387689.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [284/500], Step [1/1], Loss: 387961.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [285/500], Step [1/1], Loss: 388230.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [286/500], Step [1/1], Loss: 388496.6875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [287/500], Step [1/1], Loss: 388760.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [288/500], Step [1/1], Loss: 389020.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [289/500], Step [1/1], Loss: 389278.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [290/500], Step [1/1], Loss: 389533.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [291/500], Step [1/1], Loss: 389786.21875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [292/500], Step [1/1], Loss: 390035.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [293/500], Step [1/1], Loss: 390283.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [294/500], Step [1/1], Loss: 390527.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [295/500], Step [1/1], Loss: 390769.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [296/500], Step [1/1], Loss: 391009.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [297/500], Step [1/1], Loss: 391245.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [298/500], Step [1/1], Loss: 391480.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [299/500], Step [1/1], Loss: 391712.34375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [300/500], Step [1/1], Loss: 391941.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [301/500], Step [1/1], Loss: 392168.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [302/500], Step [1/1], Loss: 392393.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [303/500], Step [1/1], Loss: 392616.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [304/500], Step [1/1], Loss: 392836.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [305/500], Step [1/1], Loss: 393053.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [306/500], Step [1/1], Loss: 393269.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [307/500], Step [1/1], Loss: 393482.28125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [308/500], Step [1/1], Loss: 393693.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [309/500], Step [1/1], Loss: 393901.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [310/500], Step [1/1], Loss: 394108.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [311/500], Step [1/1], Loss: 394313.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [312/500], Step [1/1], Loss: 394515.21875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [313/500], Step [1/1], Loss: 394715.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [314/500], Step [1/1], Loss: 394913.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [315/500], Step [1/1], Loss: 395109.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [316/500], Step [1/1], Loss: 395303.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [317/500], Step [1/1], Loss: 395495.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [318/500], Step [1/1], Loss: 395684.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [319/500], Step [1/1], Loss: 395872.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [320/500], Step [1/1], Loss: 396058.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [321/500], Step [1/1], Loss: 396243.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [322/500], Step [1/1], Loss: 396425.03125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [323/500], Step [1/1], Loss: 396605.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [324/500], Step [1/1], Loss: 396783.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [325/500], Step [1/1], Loss: 396960.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [326/500], Step [1/1], Loss: 397134.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [327/500], Step [1/1], Loss: 397307.71875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [328/500], Step [1/1], Loss: 397478.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [329/500], Step [1/1], Loss: 397648.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [330/500], Step [1/1], Loss: 397815.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [331/500], Step [1/1], Loss: 397981.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [332/500], Step [1/1], Loss: 398145.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [333/500], Step [1/1], Loss: 398307.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [334/500], Step [1/1], Loss: 398468.3125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [335/500], Step [1/1], Loss: 398627.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [336/500], Step [1/1], Loss: 398784.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [337/500], Step [1/1], Loss: 398940.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [338/500], Step [1/1], Loss: 399094.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [339/500], Step [1/1], Loss: 399246.8125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [340/500], Step [1/1], Loss: 399397.6875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [341/500], Step [1/1], Loss: 399547.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [342/500], Step [1/1], Loss: 399694.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [343/500], Step [1/1], Loss: 399841.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [344/500], Step [1/1], Loss: 399985.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [345/500], Step [1/1], Loss: 400129.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [346/500], Step [1/1], Loss: 400271.03125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [347/500], Step [1/1], Loss: 400411.34375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [348/500], Step [1/1], Loss: 400550.28125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [349/500], Step [1/1], Loss: 400687.8125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [350/500], Step [1/1], Loss: 400823.96875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [351/500], Step [1/1], Loss: 400958.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [352/500], Step [1/1], Loss: 401091.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [353/500], Step [1/1], Loss: 401223.6875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [354/500], Step [1/1], Loss: 401354.3125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [355/500], Step [1/1], Loss: 401483.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [356/500], Step [1/1], Loss: 401611.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [357/500], Step [1/1], Loss: 401737.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [358/500], Step [1/1], Loss: 401862.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [359/500], Step [1/1], Loss: 401986.875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [360/500], Step [1/1], Loss: 402109.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [361/500], Step [1/1], Loss: 402231.03125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [362/500], Step [1/1], Loss: 402351.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [363/500], Step [1/1], Loss: 402470.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [364/500], Step [1/1], Loss: 402587.8125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [365/500], Step [1/1], Loss: 402704.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [366/500], Step [1/1], Loss: 402819.6875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [367/500], Step [1/1], Loss: 402933.96875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [368/500], Step [1/1], Loss: 403046.8125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [369/500], Step [1/1], Loss: 403158.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [370/500], Step [1/1], Loss: 403269.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [371/500], Step [1/1], Loss: 403378.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [372/500], Step [1/1], Loss: 403487.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [373/500], Step [1/1], Loss: 403594.4375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [374/500], Step [1/1], Loss: 403700.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [375/500], Step [1/1], Loss: 403805.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [376/500], Step [1/1], Loss: 403910.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [377/500], Step [1/1], Loss: 404013.03125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [378/500], Step [1/1], Loss: 404115.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [379/500], Step [1/1], Loss: 404215.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [380/500], Step [1/1], Loss: 404315.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [381/500], Step [1/1], Loss: 404414.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [382/500], Step [1/1], Loss: 404512.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [383/500], Step [1/1], Loss: 404609.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [384/500], Step [1/1], Loss: 404705.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [385/500], Step [1/1], Loss: 404800.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [386/500], Step [1/1], Loss: 404893.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [387/500], Step [1/1], Loss: 404987.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [388/500], Step [1/1], Loss: 405078.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [389/500], Step [1/1], Loss: 405170.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [390/500], Step [1/1], Loss: 405260.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [391/500], Step [1/1], Loss: 405349.3125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [392/500], Step [1/1], Loss: 405437.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [393/500], Step [1/1], Loss: 405525.03125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [394/500], Step [1/1], Loss: 405611.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [395/500], Step [1/1], Loss: 405696.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [396/500], Step [1/1], Loss: 405781.6875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [397/500], Step [1/1], Loss: 405865.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [398/500], Step [1/1], Loss: 405948.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [399/500], Step [1/1], Loss: 406031.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [400/500], Step [1/1], Loss: 406112.5, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [401/500], Step [1/1], Loss: 406193.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [402/500], Step [1/1], Loss: 406272.6875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [403/500], Step [1/1], Loss: 406351.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [404/500], Step [1/1], Loss: 406429.65625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [405/500], Step [1/1], Loss: 406507.03125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [406/500], Step [1/1], Loss: 406583.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [407/500], Step [1/1], Loss: 406659.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [408/500], Step [1/1], Loss: 406734.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [409/500], Step [1/1], Loss: 406808.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [410/500], Step [1/1], Loss: 406881.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [411/500], Step [1/1], Loss: 406954.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [412/500], Step [1/1], Loss: 407026.6875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [413/500], Step [1/1], Loss: 407097.875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [414/500], Step [1/1], Loss: 407168.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [415/500], Step [1/1], Loss: 407237.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [416/500], Step [1/1], Loss: 407307.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [417/500], Step [1/1], Loss: 407375.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [418/500], Step [1/1], Loss: 407443.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [419/500], Step [1/1], Loss: 407510.1875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [420/500], Step [1/1], Loss: 407576.5, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [421/500], Step [1/1], Loss: 407642.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [422/500], Step [1/1], Loss: 407707.21875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [423/500], Step [1/1], Loss: 407771.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [424/500], Step [1/1], Loss: 407835.3125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [425/500], Step [1/1], Loss: 407898.125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [426/500], Step [1/1], Loss: 407960.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [427/500], Step [1/1], Loss: 408022.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [428/500], Step [1/1], Loss: 408083.28125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [429/500], Step [1/1], Loss: 408143.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [430/500], Step [1/1], Loss: 408203.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [431/500], Step [1/1], Loss: 408263.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [432/500], Step [1/1], Loss: 408321.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [433/500], Step [1/1], Loss: 408379.875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [434/500], Step [1/1], Loss: 408437.21875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [435/500], Step [1/1], Loss: 408494.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [436/500], Step [1/1], Loss: 408550.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [437/500], Step [1/1], Loss: 408606.34375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [438/500], Step [1/1], Loss: 408661.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [439/500], Step [1/1], Loss: 408716.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [440/500], Step [1/1], Loss: 408770.3125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [441/500], Step [1/1], Loss: 408823.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [442/500], Step [1/1], Loss: 408876.875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [443/500], Step [1/1], Loss: 408929.28125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [444/500], Step [1/1], Loss: 408981.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [445/500], Step [1/1], Loss: 409032.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [446/500], Step [1/1], Loss: 409083.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [447/500], Step [1/1], Loss: 409133.8125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [448/500], Step [1/1], Loss: 409183.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [449/500], Step [1/1], Loss: 409232.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [450/500], Step [1/1], Loss: 409281.75, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [451/500], Step [1/1], Loss: 409330.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [452/500], Step [1/1], Loss: 409378.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [453/500], Step [1/1], Loss: 409425.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [454/500], Step [1/1], Loss: 409472.53125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [455/500], Step [1/1], Loss: 409519.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [456/500], Step [1/1], Loss: 409564.875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [457/500], Step [1/1], Loss: 409610.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [458/500], Step [1/1], Loss: 409655.5, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [459/500], Step [1/1], Loss: 409700.25, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [460/500], Step [1/1], Loss: 409744.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [461/500], Step [1/1], Loss: 409787.9375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [462/500], Step [1/1], Loss: 409831.3125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [463/500], Step [1/1], Loss: 409874.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [464/500], Step [1/1], Loss: 409916.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [465/500], Step [1/1], Loss: 409958.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [466/500], Step [1/1], Loss: 409999.78125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [467/500], Step [1/1], Loss: 410041.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [468/500], Step [1/1], Loss: 410081.6875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [469/500], Step [1/1], Loss: 410122.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [470/500], Step [1/1], Loss: 410161.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [471/500], Step [1/1], Loss: 410201.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [472/500], Step [1/1], Loss: 410240.40625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [473/500], Step [1/1], Loss: 410279.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [474/500], Step [1/1], Loss: 410317.3125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [475/500], Step [1/1], Loss: 410355.4375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [476/500], Step [1/1], Loss: 410393.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [477/500], Step [1/1], Loss: 410430.125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [478/500], Step [1/1], Loss: 410467.03125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [479/500], Step [1/1], Loss: 410503.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [480/500], Step [1/1], Loss: 410539.34375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [481/500], Step [1/1], Loss: 410575.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [482/500], Step [1/1], Loss: 410610.46875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [483/500], Step [1/1], Loss: 410645.15625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [484/500], Step [1/1], Loss: 410679.84375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [485/500], Step [1/1], Loss: 410714.21875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [486/500], Step [1/1], Loss: 410748.0, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [487/500], Step [1/1], Loss: 410781.59375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [488/500], Step [1/1], Loss: 410814.625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [489/500], Step [1/1], Loss: 410847.5625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [490/500], Step [1/1], Loss: 410880.09375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [491/500], Step [1/1], Loss: 410912.4375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [492/500], Step [1/1], Loss: 410944.5, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [493/500], Step [1/1], Loss: 410976.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [494/500], Step [1/1], Loss: 411007.3125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [495/500], Step [1/1], Loss: 411038.21875, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [496/500], Step [1/1], Loss: 411068.90625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [497/500], Step [1/1], Loss: 411099.375, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [498/500], Step [1/1], Loss: 411129.28125, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [499/500], Step [1/1], Loss: 411159.0625, Test Accuracy: 34.21052631578947%\n",
      "{i+1}Epoch [500/500], Step [1/1], Loss: 411188.25, Test Accuracy: 34.21052631578947%\n"
     ]
    }
   ],
   "source": [
    "#use FD to optimize The Neural network\n",
    "n_epochs =500\n",
    "NN_IRIS = Neural_Net(input_size=4, hidden_size=10, n_classes=3)\n",
    "N_dim = NN_IRIS.count_parameters()\n",
    "grad_dim = 50\n",
    "\n",
    "#specify we don't need the computation graph to keep track of the gradients, we will use SPSA to update the weights\n",
    "with torch.no_grad():\n",
    "    for param in NN_IRIS.parameters():\n",
    "        param.requires_grad = False\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# learning parameters\n",
    "\n",
    "init_pos = NN_IRIS.get_params()\n",
    "\n",
    "if init_pos.requires_grad:\n",
    "    # Detach the tensor from the computation graph\n",
    "    init_pos = init_pos.detach()\n",
    "if init_pos.is_cuda:\n",
    "    # Move the tensor to the CPU\n",
    "    init_pos = init_pos.cpu()\n",
    "init_pos = init_pos.numpy()\n",
    "\n",
    "FD_optimizer = FD_opt(init_pos,n_perturb=grad_dim,alpha=1e-3,epsilon=1e-7)\n",
    "Adam = AdamOptimizer(init_pos, lr=1e-2, beta1=0.99, beta2=0.7, epsilon=1e-8)\n",
    "\n",
    "test_acc_FD, best_reward_FD = train_online_FD_NN(NN_IRIS,N_dim, n_epochs,  Iris_train_loader, Iris_test_loader, loss, FD_optimizer,Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "PEPG",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          5.2631578947368425,
          0,
          0,
          18.42105263157895,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          21.05263157894737,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316,
          44.73684210526316
         ]
        }
       ],
       "layout": {
        "height": 300,
        "margin": {
         "b": 20,
         "l": 20,
         "r": 20,
         "t": 20
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "width": 400,
        "xaxis": {
         "title": {
          "text": "Epochs"
         },
         "type": "log"
        },
        "yaxis": {
         "title": {
          "text": "Accuracy [%]"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(test_acc_FD)), y=test_acc_FD, mode='lines', name='PEPG'))\n",
    "#change theme to white and set the sizer of the plot\n",
    "fig.update_layout(template='plotly_white', width=400, height=300,margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig.update_xaxes(title_text=\"Epochs\",type = 'log')\n",
    "fig.update_yaxes(title_text=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using PSO for training the NN\n",
    "n_epochs =1\n",
    "NN_MNIST = Tiny_convnet()\n",
    "N_dim = NN_MNIST.count_parameters()\n",
    "pop_size = 100\n",
    "#specify we don't need the computation graph to keep track of the gradients, we will use CMAES to update the weights\n",
    "with torch.no_grad():\n",
    "    for param in NN_MNIST.parameters():\n",
    "        param.requires_grad = False\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# learning parameters\n",
    "\n",
    "init_pos = NN_MNIST.get_params()\n",
    "\n",
    "if init_pos.requires_grad:\n",
    "    # Detach the tensor from the computation graph\n",
    "    init_pos = init_pos.detach()\n",
    "if init_pos.is_cuda:\n",
    "    # Move the tensor to the CPU\n",
    "    init_pos = init_pos.cpu()\n",
    "init_pos = init_pos.numpy()\n",
    "\n",
    "#params dictionary\n",
    "upper_bound = 0.5\n",
    "lower_bound = -0.5\n",
    "\n",
    "params = {'c_1': 2.5, \n",
    "          'c_2': 0.85,\n",
    "          'w': 0.7,\n",
    "          'Vmax': 0.15*(upper_bound-lower_bound),\n",
    "          'upper_bound': upper_bound,\n",
    "          'lower_bound': lower_bound,\n",
    "          'pop_size' :pop_size,\n",
    "          }\n",
    "\n",
    "init_pos = (upper_bound - lower_bound) * np.random.rand(N_dim, pop_size) + lower_bound\n",
    "V_init = 0.1 * np.random.rand(N_dim, pop_size)\n",
    "PSO_optimizer = PSO_opt(X_init = init_pos,V_init = V_init,params=params)\n",
    "\n",
    "test_acc_PSO,best_reward_PSO = train_online_pop_NN(NN_MNIST, n_epochs, train_loader_MNIST, test_loader_MNIST, loss, PSO_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(test_acc_PSO)), y=test_acc_PSO, mode='lines', name='PEPG'))\n",
    "#change theme to white and set the sizer of the plot\n",
    "fig.update_layout(template='plotly_white', width=400, height=300,margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig.update_xaxes(title_text=\"Epochs\",type = 'log')\n",
    "fig.update_yaxes(title_text=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_MNIST\n",
    "\n",
    "# Initialize a list to store the figures\n",
    "figs = []\n",
    "\n",
    "# Iterate through each model parameter\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:  # Filter out only weight parameters\n",
    "        # Flatten the weights\n",
    "        weights = param.detach().cpu().numpy().flatten()\n",
    "        \n",
    "        # Create a histogram for the weights\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Histogram(x=weights, name=name))\n",
    "        \n",
    "        # Update layout to add titles and improve readability\n",
    "        fig.update_layout(\n",
    "            title=f'Histogram of Weights for Layer: {name}',\n",
    "            xaxis_title='Weight values',\n",
    "            yaxis_title='Frequency',\n",
    "            bargap=0.2\n",
    "        )\n",
    "        \n",
    "        # Append the figure to the list\n",
    "        figs.append(fig)\n",
    "\n",
    "# Show all histograms\n",
    "for fig in figs:\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wine dataset\n",
    "wine_df = pd.read_csv(\"data\\\\WINE\\\\winequality-red.csv\")\n",
    "\n",
    "wine_raw = wine_df.values.astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.from_numpy(wine_raw[:, :-1])\n",
    "Y = torch.from_numpy(wine_raw[:, -1]).unsqueeze(1)\n",
    "\n",
    "# Create a single dataset\n",
    "full_dataset = Custom_dataset(X, Y)\n",
    "\n",
    "# Split into train and test sets, first set the size of the split\n",
    "train_size = int(0.75 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "# split into train and test sets using pytorch randomsplit\n",
    "\n",
    "Wine_train, Wine_test = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "Wine_train_loader = torch.utils.data.DataLoader(dataset=Wine_train, batch_size=100, shuffle=True)\n",
    "Wine_test_loader = torch.utils.data.DataLoader(dataset=Wine_test, batch_size=100, shuffle=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
